{"cells":[{"cell_type":"markdown","metadata":{"id":"XTzBUFWQ-OWj"},"source":["# Exploring Advanced Prompt Engineering Patterns\n","\n","In this notebook you will use ChatGPT and LangChain to learn about:\n","\n","- Chain of Thought Pattern\n","- Self-Consistency Pattern\n","- Least to Most Pattern\n","- ReAct Pattern\n","\n","___Created By: Dipanjan (DJ)___"]},{"cell_type":"markdown","metadata":{"id":"L1KvMtf54l0d"},"source":["## Install OpenAI and LangChain dependencies\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eEzecGfICCw1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64e14af7-358d-4f66-c004-c6d677b8baf1","executionInfo":{"status":"ok","timestamp":1734090851310,"user_tz":-330,"elapsed":43727,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain==0.3.10 in /usr/local/lib/python3.10/dist-packages (0.3.10)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (3.11.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.22 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.22)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.3.2)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.10.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.10) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.18.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.10) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain==0.3.10) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.2.2)\n","Collecting langchain-openai==0.2.12\n","  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai==0.2.12) (0.3.22)\n","Collecting openai<2.0.0,>=1.55.3 (from langchain-openai==0.2.12)\n","  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.12)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.1.147)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (24.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.10.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (9.0.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (4.66.6)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2024.9.11)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.55.3->langchain-openai==0.2.12) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai==0.2.12) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.2.12) (2.2.3)\n","Downloading langchain_openai-0.2.12-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.2/390.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.54.5\n","    Uninstalling openai-1.54.5:\n","      Successfully uninstalled openai-1.54.5\n","Successfully installed langchain-openai-0.2.12 openai-1.57.3 tiktoken-0.8.0\n","Collecting langchain-community==0.3.11\n","  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (3.11.10)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.11)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community==0.3.11)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain<0.4.0,>=0.3.11 (from langchain-community==0.3.11)\n","  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community==0.3.11)\n","  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (0.1.147)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (1.26.4)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.11)\n","  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.11) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.11) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.3.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.10.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.11)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.11) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.11) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community==0.3.11) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community==0.3.11) (2.27.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.11)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community==0.3.11) (1.2.2)\n","Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.22\n","    Uninstalling langchain-core-0.3.22:\n","      Successfully uninstalled langchain-core-0.3.22\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.10\n","    Uninstalling langchain-0.3.10:\n","      Successfully uninstalled langchain-0.3.10\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n","Collecting duckduckgo-search==6.3.5\n","  Downloading duckduckgo_search-6.3.5-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search==6.3.5) (8.1.7)\n","Collecting primp>=0.6.5 (from duckduckgo-search==6.3.5)\n","  Downloading primp-0.8.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading duckduckgo_search-6.3.5-py3-none-any.whl (27 kB)\n","Downloading primp-0.8.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n","Successfully installed duckduckgo-search-6.3.5 primp-0.8.3\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"]}],"source":["!pip install langchain==0.3.10\n","!pip install langchain-openai==0.2.12\n","!pip install langchain-community==0.3.11\n","!pip install duckduckgo-search==6.3.5\n","!pip install beautifulsoup4"]},{"cell_type":"markdown","metadata":{"id":"CiwGjVWK4q6F"},"source":["## Load OpenAI API Credentials\n","\n","Here we load it from get password function"]},{"cell_type":"markdown","metadata":{"id":"PtBa7rlWJWH3"},"source":["## Enter API Tokens"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9ogxBkS6ZnnC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80bbb7cc-67c5-40ec-b2f4-341d57b34226","executionInfo":{"status":"ok","timestamp":1734090878545,"user_tz":-330,"elapsed":4131,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Open AI API Key: ··········\n"]}],"source":["from getpass import getpass\n","\n","OPENAI_KEY = getpass('Enter Open AI API Key: ')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"kDe44J0N0NcC","executionInfo":{"status":"ok","timestamp":1734090880296,"user_tz":-330,"elapsed":324,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"outputs":[],"source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = OPENAI_KEY"]},{"cell_type":"markdown","metadata":{"id":"VDWhgxCy5bA6"},"source":["## Load Necessary Dependencies and ChatGPT LLM"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"mY2bapqfuWq1","executionInfo":{"status":"ok","timestamp":1734090885062,"user_tz":-330,"elapsed":3563,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","chatgpt = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)"]},{"cell_type":"markdown","source":["## Chain-of-Thought (CoT) Pattern"],"metadata":{"id":"YvjOf7W_v35E"}},{"cell_type":"markdown","source":["### Zero Shot CoT"],"metadata":{"id":"I7GyfJf_srzS"}},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","prompt = \"\"\"Given the following game details,\n","            How many yards was the shortest valid field goal?\n","            Think step-by-step to get to your answer.\n","            Show your step-by-step reasoning and then the answer.\n","\n","            Coming off their win over the Broncos, the Redskins flew to Cowboys Stadium\n","            for their Week 11 NFC East rivalry match against the Dallas Cowboys.\n","            After a scoreless first quarter, Washington would strike in the second quarter\n","            as kicker Shaun Suisham nailed a 35-yard field goal.\n","            The Redskins would try to add onto their lead in the third quarter\n","            with Suisham booting a 31-yard field goal which was blocked.\n","            However, in the fourth quarter, the Cowboys rallied as quarterback Tony Romo\n","            completing a 10-yard touchdown pass to wide receiver Patrick Crayton.\n","         \"\"\"\n","\n","response = chatgpt.invoke(prompt)\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"zW1h6eeEv57r","outputId":"cc007be7-9ac3-4ebb-93fb-faa94b00e9b4","executionInfo":{"status":"ok","timestamp":1734090247187,"user_tz":-330,"elapsed":9380,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine the shortest valid field goal in the context of the game details provided, we need to analyze the information step-by-step.\n\n1. **Understanding Field Goals**: A field goal in American football is a scoring play where the ball is kicked through the opponent's goalposts. The distance of a field goal is measured from the line of scrimmage to the goalposts, plus an additional 10 yards for the end zone.\n\n2. **Field Goal Distances Mentioned**: In the game details:\n   - Shaun Suisham successfully made a 35-yard field goal.\n   - He attempted a 31-yard field goal, but it was blocked.\n\n3. **Valid Field Goals**: A valid field goal must be successfully kicked through the goalposts. Since the 31-yard attempt was blocked, it does not count as a valid field goal.\n\n4. **Identifying the Shortest Valid Field Goal**: The only successful field goal mentioned is the 35-yard field goal. Since the blocked attempt does not count, we need to consider if there are any other field goals mentioned or implied.\n\n5. **Conclusion**: Since the only successful field goal mentioned is the 35-yard field goal, and there are no other successful field goals listed, we conclude that the shortest valid field goal in this context is the one that was successfully made.\n\nThus, the answer is:\n\n**35 yards**."},"metadata":{}}]},{"cell_type":"markdown","source":["### Few-Shot CoT"],"metadata":{"id":"HS-RDO8jFqqM"}},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","prompt = \"\"\"Given the following game details,\n","            How many yards was the shortest valid field goal?\n","            Think step-by-step to get to your answer similar to the following examples:\n","\n","            Example 1:\n","            Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n","               Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","            Reasoning:\n","              Step 1: Roger started with 5 balls.\n","              Step 2: 2 cans of 3 tennis balls each is 6 tennis balls in total.\n","              Step 3: 5 + 6 = 11 tennis balls.\n","\n","            A: 11 Tennis Balls\n","\n","            Example 2:\n","            Q: How many keystrokes are needed to type the numbers from 1 to 500?\n","\n","            Reasoning:\n","              Step 1: There are 9 one-digit numbers from 1 to 9.\n","              Step 2: There are 90 two-digit numbers from 10 to 99.\n","              Step 3: There are 401 three-digit numbers from 100 to 500.\n","              Step 4: Adding all of them up, 9 + 90 + 401 = 1392 keystrokes.\n","\n","            A: 1392 keystrokes\n","\n","\n","            Q: Coming off their win over the Broncos, the Redskins flew to Cowboys Stadium\n","            for their Week 11 NFC East rivalry match against the Dallas Cowboys.\n","            After a scoreless first quarter, Washington would strike in the second quarter\n","            as kicker Shaun Suisham nailed a 35-yard field goal.\n","            The Redskins would try to add onto their lead in the third quarter\n","            with Suisham booting a 31-yard field goal which was blocked.\n","            However, in the fourth quarter, the Cowboys rallied as quarterback Tony Romo\n","            completing a 10-yard touchdown pass to wide receiver Patrick Crayton.\n","         \"\"\"\n","\n","response = chatgpt.invoke(prompt)\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"99647915-0953-4316-f010-4946e31392b2","id":"h0HnrM2SFqqM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["To determine the shortest valid field goal from the game details provided, we can analyze the information step-by-step.\n","\n","Reasoning:\n","Step 1: The first field goal mentioned is a successful 35-yard field goal made by Shaun Suisham in the second quarter.\n","Step 2: The second field goal attempt is a 31-yard field goal by Suisham, but it was blocked and therefore does not count as a valid field goal.\n","Step 3: Since the only successful field goal mentioned is the 35-yard field goal, we need to consider if there are any other field goals mentioned in the details.\n","Step 4: There are no other field goals mentioned, so the shortest valid field goal is the one that was successfully made.\n","\n","A: 35 yards\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tgxPaWv_MiqF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Self-Consistency Pattern"],"metadata":{"id":"7kcqGa4kMi7l"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","cot_prompt_txt = \"\"\"Given the following problem,\n","                    Think step-by-step to get to your answer similar to the following example:\n","\n","                    Example 1:\n","                    Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n","                    Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","                    Reasoning:\n","                    Step 1: Roger started with 5 balls.\n","                    Step 2: 2 cans of 3 tennis balls each is 6 tennis balls in total.\n","                    Step 3: 5 + 6 = 11 tennis balls.\n","\n","                    A: 11 Tennis Balls\n","\n","                    Problem:\n","                    {problem}\n","                \"\"\"\n","\n","cot_prompt = ChatPromptTemplate.from_template(cot_prompt_txt)\n","cot_prompt.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"63b840cc-fdda-4dc1-96f9-060da30ba5c7","id":"vH1IZD6PMi7l"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","Given the following problem,\n","                    Think step-by-step to get to your answer similar to the following example:\n","\n","                    Example 1:\n","                    Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. \n","                    Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","                    Reasoning: \n","                    Step 1: Roger started with 5 balls. \n","                    Step 2: 2 cans of 3 tennis balls each is 6 tennis balls in total. \n","                    Step 3: 5 + 6 = 11 tennis balls.\n","                    \n","                    A: 11 Tennis Balls\n","\n","                    Problem:\n","                    \u001b[33;1m\u001b[1;3m{problem}\u001b[0m\n","                \n"]}]},{"cell_type":"code","source":["self_con_prompt_txt = \"\"\"Given the following problem\n","                         and 3 diverse reasoning paths explored by an AI model\n","                         analyse these pathways carefully,\n","                         aggregate, take the majority vote as needed\n","                         and generate a final single reasoning path along with the answer\n","\n","                         AI Model Reasoning Path 1:\n","                         {reasoning_path_1}\n","\n","                         AI Model Reasoning Path 2:\n","                         {reasoning_path_2}\n","\n","                         AI Model Reasoning Path 3:\n","                         {reasoning_path_3}\n","\n","                         Problem:\n","                         {problem}\n","                    \"\"\"\n","\n","self_con_prompt = ChatPromptTemplate.from_template(self_con_prompt_txt)\n","self_con_prompt.pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jN1SckLsNrox","outputId":"82510f27-d2b9-4671-cc07-cbfe7b983d76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n","Given the following problem \n","                         and 3 diverse reasoning paths explored by an AI model\n","                         analyse these pathways carefully, \n","                         aggregate, take the majority vote as needed \n","                         and generate a final single reasoning path along with the answer\n","\n","                         AI Model Reasoning Path 1:\n","                         \u001b[33;1m\u001b[1;3m{reasoning_path_1}\u001b[0m\n","\n","                         AI Model Reasoning Path 2:\n","                         \u001b[33;1m\u001b[1;3m{reasoning_path_2}\u001b[0m\n","\n","                         AI Model Reasoning Path 3:\n","                         \u001b[33;1m\u001b[1;3m{reasoning_path_3}\u001b[0m\n","\n","                         Problem:\n","                         \u001b[33;1m\u001b[1;3m{problem}\u001b[0m\n","                    \n"]}]},{"cell_type":"code","source":["# do CoT exploration independently 3 times with these models\n","gpt1 = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n","gpt2 = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n","gpt3 = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)"],"metadata":{"id":"XhrBicOHNgtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# do final reasoning and aggregation with this model\n","chatgpt = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)"],"metadata":{"id":"HIOdfK0xOwyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","\n","# independent CoT explorations with 3 different LLMs (with different temp settings)\n","llm_chain1 = cot_prompt | gpt1 | StrOutputParser()\n","llm_chain2 = cot_prompt | gpt2 | StrOutputParser()\n","llm_chain3 = cot_prompt | gpt3 | StrOutputParser()\n","\n","sc_chain = (\n","                RunnableParallel(problem=RunnablePassthrough(),\n","                                 reasoning_path_1=llm_chain1,\n","                                 reasoning_path_2=llm_chain2,\n","                                 reasoning_path_3=llm_chain3)\n","                        |\n","                self_con_prompt\n","                        |\n","                    chatgpt\n","            )"],"metadata":{"id":"Y8f8hdCFOi1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"\"\"Q: Coming off their win over the Broncos, the Redskins flew to Cowboys Stadium\n","            for their Week 11 NFC East rivalry match against the Dallas Cowboys.\n","            After a scoreless first quarter, Washington would strike in the second quarter\n","            as kicker Shaun Suisham nailed a 35-yard field goal.\n","            The Redskins would try to add onto their lead in the third quarter\n","            with Suisham booting a 31-yard field goal which was blocked.\n","            However, in the fourth quarter, the Cowboys rallied as quarterback Tony Romo\n","            completing a 10-yard touchdown pass to wide receiver Patrick Crayton.\n","            How many yards was the shortest valid field goal?\n","           \"\"\""],"metadata":{"id":"USIGTgvvNLbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = sc_chain.invoke({'problem': question})\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"id":"U2IIjPhLQuCD","outputId":"da80fc8d-530a-4403-d0a3-fe6dc8ed1747"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine the shortest valid field goal based on the reasoning paths provided, let's analyze the key points from each pathway:\n\n1. **Path 1** concludes that the shortest valid field goal is the 31-yard attempt, as it is mentioned as a valid field goal attempt.\n2. **Path 2** states that the only successful field goal is the 35-yard field goal, as the 31-yard attempt was blocked and therefore not valid.\n3. **Path 3** also concludes that the shortest valid field goal is the 35-yard field goal, emphasizing that the 31-yard attempt was blocked and thus not counted as valid.\n\nFrom the analysis:\n- **Paths 2 and 3** agree that the 35-yard field goal is the shortest valid field goal since the 31-yard attempt was blocked and not successful.\n- **Path 1** incorrectly identifies the 31-yard attempt as valid despite it being blocked.\n\nGiven that the majority of the reasoning paths (2 and 3) conclude that the shortest valid field goal is the 35-yard field goal, we can aggregate this information.\n\n### Final Reasoning Path:\n1. The problem states that Shaun Suisham successfully kicked a 35-yard field goal in the second quarter.\n2. It also mentions that he attempted a 31-yard field goal in the third quarter, but that kick was blocked and therefore not valid.\n3. Since the only successful field goal mentioned is the 35-yard field goal, this is the shortest valid field goal.\n\n### Final Answer:\n**35 yards**"},"metadata":{}}]},{"cell_type":"markdown","source":["## Least to Most Prompting"],"metadata":{"id":"Hmmg7blAvcAt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QePjecVqviA3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fef3da1-013f-48c1-e1ae-ce5a4bbd0c61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Let's break down this problem into subproblems:\n","\n","1. Identify the valid field goals attempted in the game.\n","2. Determine the lengths of these valid field goals.\n","3. Find the shortest valid field goal from the identified lengths.\n","\n","Now, let's analyze each subproblem:\n","\n","1. The valid field goals attempted in the game are:\n","   - Shaun Suisham's successful 35-yard field goal in the second quarter.\n","   - The 31-yard field goal attempted by Suisham in the third quarter, which was blocked and therefore not valid.\n","\n","2. The lengths of the valid field goals are:\n","   - 35 yards (successful field goal).\n","\n","3. Since the only valid field goal is 35 yards, it is also the shortest valid field goal.\n","\n","A: The final answer is 35 yards.\n"]}],"source":["PROMPT = \"\"\"You are a strong reasoning agent.\n","            Respond to user question with concise and helpful information.\n","            Follow the format as mentioned in the example,\n","            break down the problem first into sub problems,\n","            answer each sub problem sequentially, analyze it\n","            and then get to the final answer.\n","            Do not jump directly to the answer\n","\n","            Example:\n","\n","            Q: Against Tim Tebow and the Broncos, the two teams would be in a 0-0 deadlock\n","            in the first half, though the Broncos would nearly score in the second quarter\n","            on a 28-yard field goal, which would then be blocked by Julius Peppers.\n","            The Bears would then score 10 points on Marion Barber's 9-yard touchdown run,\n","            and Robbie Gould's team record-breaking 57-yard field goal, but Tebow's touchdown pass\n","            to Demaryius Thomas and Matt Prater's 59-yard field goal would tie the game.\n","            Barber would commit two costly mistakes during the late portion of the game.\n","            In the fourth quarter, Barber would run out of bounds with 1:55 left,\n","            and Barber would also fumble in overtime.\n","            The Broncos would then move downfield and kick a game-winning field goal.\n","            How many yards was the games longest field goal?\n","\n","            Response:\n","            Let's break down this problem into subproblems:\n","              1. What were the field goals when field goal was?\n","              2. How many yards was the games longest field goal?\n","\n","            The answer to subproblems are as follows:\n","              1. The field goals were 28-yard,  57-yard and  59-yard which are all valid goals\n","              2. The maximum value out of  59-yard, 28-yard and 57-yard is 59\n","\n","            A: The final answer is 59 yards.\n","\n","            Question:\n","            {query}\n","        \"\"\"\n","\n","query = \"\"\"Coming off their win over the Broncos, the Redskins flew to Cowboys Stadium\n","           for their Week 11 NFC East rivalry match against the Dallas Cowboys.\n","           After a scoreless first quarter, Washington would strike in the second quarter\n","           as kicker Shaun Suisham nailed a 35-yard field goal.\n","           The Redskins would try to add onto their lead in the third quarter\n","           with Suisham booting a 31-yard field goal which was blocked.\n","           However, in the fourth quarter, the Cowboys rallied as quarterback Tony Romo\n","           completing a 10-yard touchdown pass to wide receiver Patrick Crayton.\n","           How many yards was the shortest valid field goal?\n","        \"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(PROMPT)\n","\n","chain = (prompt\n","           |\n","         chatgpt\n",")\n","\n","response = chain.invoke({\"query\": query})\n","print(response.content)"]},{"cell_type":"markdown","source":["## ReAct Prompting - Agentic AI Flow"],"metadata":{"id":"op_BboJFoXg5"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"6959415a-6939-4ac1-da5f-e9c8e3e93f6a","id":"7GsAHiKvoXg5","executionInfo":{"status":"ok","timestamp":1734090338992,"user_tz":-330,"elapsed":3791,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"As of my last knowledge update in October 2023, there is no widely recognized tool or framework specifically named \"LangGraph.\" It's possible that it could refer to a new project, library, or concept that has emerged after that date, or it may be a niche tool that hasn't gained significant attention in mainstream discussions.\n\nIf you have specific details or context about LangGraph, such as its purpose or the domain it operates in (e.g., natural language processing, graph databases, etc.), I could help you better understand related concepts or technologies. Otherwise, I recommend checking the latest resources or official documentation for the most accurate and up-to-date information."},"metadata":{}}],"source":["prompt = \"\"\"Tell me about LangGraph in detail including who made it and key features.\n","            Do not make up answers, only answer if you are sure\"\"\"\n","\n","# it will either not answer or end up hallucinating the answer\n","response = chatgpt.invoke(prompt)\n","display(Markdown(response.content))"]},{"cell_type":"code","source":["from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n","from langchain_community.tools import DuckDuckGoSearchResults\n","from langchain_core.tools import tool\n","from bs4 import BeautifulSoup\n","import requests\n","from tqdm import tqdm\n","\n","@tool\n","def search_web(query: str) -> list:\n","    \"\"\"Search the web for a query.\"\"\"\n","    print('Searching web and extracting page info')\n","    wrapper = DuckDuckGoSearchAPIWrapper(max_results=5)\n","    results = wrapper.results(query, max_results=5)\n","    docs = []\n","    for result in tqdm(results):\n","        # Sending a request to the URL\n","        response = requests.get(result['link'])\n","        # Parsing the page content\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","        # Extracting all text content from the page\n","        text_content = soup.get_text(separator=\"\\n\", strip=True)\n","        docs.append(text_content)\n","    return docs"],"metadata":{"id":"Gm1cOqAnoks4","executionInfo":{"status":"ok","timestamp":1734090895959,"user_tz":-330,"elapsed":671,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["r = search_web('What is langgraph')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBI_GNDJqVbE","outputId":"12447caa-01f2-486e-85e4-91f1829054ab","executionInfo":{"status":"ok","timestamp":1734090358416,"user_tz":-330,"elapsed":3027,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-d04ddcf34a06>:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  r = search_web('What is langgraph')\n"]},{"output_type":"stream","name":"stdout","text":["Searching web and extracting page info\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:01<00:00,  2.95it/s]\n"]}]},{"cell_type":"code","source":["print(r[0][:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvd0qJn6qW3a","outputId":"b5bd00c6-d18d-4c95-d4b0-623a458ea8d6","executionInfo":{"status":"ok","timestamp":1734090360367,"user_tz":-330,"elapsed":356,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Mastering LangGraph: A Beginner's Guide to Building Intelligent Language Models | Medium\n","Open in app\n","Sign up\n","Sign in\n","Write\n","Sign up\n","Sign in\n","Introduction to LangGraph: A Beginner’s Guide\n","CPlog\n","·\n","Follow\n","3 min read\n","·\n","Feb 15, 2024\n","--\n","8\n","Listen\n","Share\n","LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, we’ll introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\n","What is LangGraph?\n","LangGraph is a library built on top of LangChain, designed to add cyclic computational capabilities to your LLM applications. While LangChain allows you to define chains of computation (Directed Acyclic Graphs or DAGs), LangGraph introduces the ability to add cycles, enabling more complex, agent-like behaviors where you can ca\n"]}]},{"cell_type":"code","source":["chatgpt = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","tools = [search_web]\n","chatgpt_with_tools = chatgpt.bind_tools(tools)\n","prompt = \"What is langgraph?\"\n","response = chatgpt_with_tools.invoke(prompt)\n","response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWvaK0l5sbvV","outputId":"8228168c-81b9-444b-8e89-77346b139ba4","executionInfo":{"status":"ok","timestamp":1734090378763,"user_tz":-330,"elapsed":749,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YeOgI8io8VgGK9u3NgUoq073', 'function': {'arguments': '{\"query\":\"langgraph\"}', 'name': 'search_web'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 49, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a7cc6023-40a9-4156-9985-638d8a5dbaf1-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langgraph'}, 'id': 'call_YeOgI8io8VgGK9u3NgUoq073', 'type': 'tool_call'}], usage_metadata={'input_tokens': 49, 'output_tokens': 15, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["response.tool_calls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lN2-oP6qsijW","outputId":"f81d4996-ed9f-40b2-d42f-92a65e76aa59","executionInfo":{"status":"ok","timestamp":1734090381167,"user_tz":-330,"elapsed":313,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'name': 'search_web',\n","  'args': {'query': 'langgraph'},\n","  'id': 'call_YeOgI8io8VgGK9u3NgUoq073',\n","  'type': 'tool_call'}]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","SYS_PROMPT = \"\"\"You run in a loop of Thought, Action, PAUSE, Observation.\n","                At the end of the loop, you output an Answer.\n","                Use Thought to describe your thoughts about the question you have been asked.\n","                Use Action to run one of the actions available to you - then return PAUSE.\n","                Observation will be the result of running those actions.\n","                For the user query below, first break it into multiple sub queries or sub steps if needed.\n","                The follow the below workflow for each sub query or sub step\n","                Analyze, validate the relevant results and then compile to generate the final answer\n","\n","                Use the following workflow format:\n","                Question: the input task you must solve\n","                Thought: you should always think about what to do\n","                Action: the action to take:\n","                          - should be either to break down the task into sub tasks first if needed\n","                          - refer to your trained knowledge or call the tool [search_web]\n","                            if you need to get additional information from the web\n","                Action Input: the input to the action\n","                Observation: the result of the action\n","                ... (this Thought/Action/Action Input/Observation can repeat N times)\n","                Thought: I now know the final answer\n","                Final Answer: the final answer to the original input question\n","             \"\"\"\n","\n","prompt_template = ChatPromptTemplate.from_messages(\n","   [\n","       (\"system\", SYS_PROMPT),\n","       (\"human\", \"Query: {query}\"),\n","       MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","   ]\n",")"],"metadata":{"id":"Km_nQy6CueVv","executionInfo":{"status":"ok","timestamp":1734090901997,"user_tz":-330,"elapsed":316,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from langchain.agents import create_tool_calling_agent\n","from langchain.agents import AgentExecutor\n","\n","tools = [search_web]\n","agent = create_tool_calling_agent(chatgpt, tools, prompt_template)\n","agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"],"metadata":{"id":"TC_SGaBPvV8f","executionInfo":{"status":"ok","timestamp":1734090905399,"user_tz":-330,"elapsed":1374,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"Tell me about LangGraph in detail including who made it and key features.\n","            Do not guess answers, only answer if you are sure\"\"\"\n","response = agent_executor.invoke({\"query\": prompt})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1AZmYhnvmMV","outputId":"a030b34a-97c8-456a-bf38-c735a76b6f00","executionInfo":{"status":"ok","timestamp":1734090923255,"user_tz":-330,"elapsed":16707,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\n","Invoking: `search_web` with `{'query': 'LangGraph details, creators, and key features'}`\n","responded: Question: Tell me about LangGraph in detail including who made it and key features.\n","Thought: I need to gather detailed information about LangGraph, including its creators and key features. Since I don't have specific information readily available, I will search the web for the most accurate and up-to-date information.\n","Action: I will perform a web search for \"LangGraph details, creators, and key features.\"\n","Action Input: \"LangGraph details, creators, and key features\"\n","Observation: I will now search for the information.\n","\n","\u001b[0mSearching web and extracting page info\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:03<00:00,  1.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36;1m\u001b[1;3m['How to Build AI Agents with LangGraph: A Step-by-Step Guide | by Lore Van Oudenhove | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nHow to Build AI Agents with LangGraph: A Step-by-Step Guide\\nLore Van Oudenhove\\n·\\nFollow\\n11 min read\\n·\\nSep 6, 2024\\n--\\n13\\nListen\\nShare\\nIntroduction\\nIn the world of AI, retrieval-augmented generation (RAG) systems have become common tools for handling simple queries and generating contextually relevant responses. However, as the demand for more sophisticated AI applications grows, there’s a need for systems that go beyond these retrieval capabilities. Enter AI agents — autonomous entities capable of performing complex, multi-step tasks, maintaining state across interactions, and dynamically adapting to new information.\\nLangGraph\\n, a powerful extension of the LangChain library, is designed to help developers build these advanced AI agents by enabling stateful, multi-actor applications with cyclic computation capabilities.\\nIn this article, we’ll explore how LangGraph transforms AI development and provide a step-by-step guide on how to build your own AI agent using an example that computes energy savings for solar panels. This example will showcase how LangGraph’s unique features can create intelligent, adaptable, and real-world-ready AI systems.\\nWhat is LangGraph?\\nLangGraph is an advanced library built on top of LangChain, designed to enhance your Large Language Model (LLM) applications by introducing cyclic computational capabilities. While LangChain allows the creation of Directed Acyclic Graphs (DAGs) for linear workflows, LangGraph takes this a step further by enabling the addition of cycles, which are essential for developing complex, agent-like behaviors. These behaviors allow LLMs to continuously loop through a process, dynamically deciding what action to take next based on evolving conditions.\\nLangraph: nodes, states, and edges.\\nAt the heart of LangGraph is the concept of a\\nstateful graph\\n:\\nState\\n: Represents the context or memory that is maintained and updated as the computation progresses. It ensures that each step in the graph can access relevant information from previous steps, allowing for dynamic decision-making based on accumulated data throughout the process.\\nNodes\\n: Serve as the building blocks of the graph, representing individual computation steps or functions. Each node performs a specific task, such as processing inputs, making decisions, or interacting with external systems. Nodes can be customized to execute a wide range of operations within the workflow.\\nEdges\\n: Connect nodes within the graph, defining the flow of computation from one step to the next. They support conditional logic, allowing the path of execution to change based on the current state and facilitate the movement of data and control between nodes, enabling complex, multi-step workflows.\\nLangGraph redefines AI development by seamlessly managing graph structure, state, and coordination, empowering the creation of sophisticated, multi-actor applications. With automatic state management, LangGraph ensures that context is preserved across interactions, enabling your AI to respond intelligently to changing inputs. Its streamlined agent coordination guarantees precise execution and efficient information exchange, letting you focus on crafting innovative workflows rather than technical intricacies. LangGraph’s flexibility allows for the development of tailored, high-performance applications, while its scalability and fault tolerance ensure your systems remain robust and reliable, even at the enterprise level.\\nStep-by-step Guide\\nNow that we have a solid understanding of what LangGraph is and how it enhances AI development, let’s dive into a practical example. In this scenario, we’ll build an AI agent designed to calculate potential energy savings for solar panels based on user input. This agent can be implemented as a lead generation tool on a solar panel seller’s website, where it interacts with potential customers, offering personalized savings estimates. By gathering key data such as monthly electricity costs, this AI agent helps educate customers on the financial benefits of solar energy while simultaneously qualifying leads for follow-up by the sales team. This example showcases the power of LangGraph in creating intelligent, dynamic systems that can automate complex tasks and drive business value.\\nStep 1: Import Necessary Libraries\\nWe start by importing all the essential Python libraries and modules required for the project.\\nfrom langchain_core.tools import tool\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import Runnable\\nfrom langchain_aws import ChatBedrock\\nimport boto3\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph.message import AnyMessage, add_messages\\nfrom langchain_core.messages import ToolMessage\\nfrom langchain_core.runnables import RunnableLambda\\nfrom langgraph.prebuilt import ToolNode\\nfrom langgraph.prebuilt import tools_condition\\nThese imports set the foundation for utilizing LangChain, LangGraph, and AWS services to build our AI assistant.\\nStep 2: Define the Tool for Calculating Solar Savings\\nNext, we define a tool that will handle the computation of energy savings based on the monthly electricity cost provided by the user.\\n@tool\\ndef compute_savings(monthly_cost: float) -> float:\\n\"\"\"\\nTool to compute the potential savings when switching to solar energy based on the user\\'s monthly electricity cost.\\nArgs:\\nmonthly_cost (float): The user\\'s current monthly electricity cost.\\nReturns:\\ndict: A dictionary containing:\\n- \\'number_of_panels\\': The estimated number of solar panels required.\\n- \\'installation_cost\\': The estimated installation cost.\\n- \\'net_savings_10_years\\': The net savings over 10 years after installation costs.\\n\"\"\"\\ndef calculate_solar_savings(monthly_cost):\\n# Assumptions for the calculation\\ncost_per_kWh = 0.28\\ncost_per_watt = 1.50\\nsunlight_hours_per_day = 3.5\\npanel_wattage = 350\\nsystem_lifetime_years = 10\\n# Monthly electricity consumption in kWh\\nmonthly_consumption_kWh = monthly_cost / cost_per_kWh\\n# Required system size in kW\\ndaily_energy_production = monthly_consumption_kWh / 30\\nsystem_size_kW = daily_energy_production / sunlight_hours_per_day\\n# Number of panels and installation cost\\nnumber_of_panels = system_size_kW * 1000 / panel_wattage\\ninstallation_cost = system_size_kW * 1000 * cost_per_watt\\n# Annual and net savings\\nannual_savings = monthly_cost * 12\\ntotal_savings_10_years = annual_savings * system_lifetime_years\\nnet_savings = total_savings_10_years - installation_cost\\nreturn {\\n\"number_of_panels\": round(number_of_panels),\\n\"installation_cost\": round(installation_cost, 2),\\n\"net_savings_10_years\": round(net_savings, 2)\\n}\\n# Return calculated solar savings\\nreturn calculate_solar_savings(monthly_cost)\\nThis function processes the user’s monthly electricity cost and returns a detailed estimate of the solar panel system’s benefits, including the number of panels required, installation costs, and net savings over ten years. For simplicity, we have made a few assumptions in the calculations, such as the average cost per kilowatt-hour and average sunlight hours. However, in a more advanced version of this AI agent, we could gather this information directly from the user, tailoring the estimates more precisely to their unique circumstances.\\nStep 3: Set Up State Management and Error Handling\\nEffective state management and error handling are crucial for building robust AI systems. Here, we define utilities to manage errors and maintain the conversation’s state.\\ndef handle_tool_error(state) -> dict:\\n\"\"\"\\nFunction to handle errors that occur during tool execution.\\nArgs:\\nstate (dict): The current state of the AI agent, which includes messages and tool call details.\\nReturns:\\ndict: A dictionary containing error messages for each tool that encountered an issue.\\n\"\"\"\\n# Retrieve the error from the current state\\nerror = state.get(\"error\")\\n# Access the tool calls from the last message in the state\\'s message history\\ntool_calls = state[\"messages\"][-1].tool_calls\\n# Return a list of ToolMessages with error details, linked to each tool call ID\\nreturn {\\n\"messages\": [\\nToolMessage(\\ncontent=f\"Error: {repr(error)}\\\\n please fix your mistakes.\",  # Format the error message for the user\\ntool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\\n)\\nfor tc in tool_calls  # Iterate over each tool call to produce individual error messages\\n]\\n}\\ndef create_tool_node_with_fallback(tools: list) -> dict:\\n\"\"\"\\nFunction to create a tool node with fallback error handling.\\nArgs:\\ntools (list): A list of tools to be included in the node.\\nReturns:\\ndict: A tool node that uses fallback behavior in case of errors.\\n\"\"\"\\n# Create a ToolNode with the provided tools and attach a fallback mechanism\\n# If an error occurs, it will invoke the handle_tool_error function to manage the error\\nreturn ToolNode(tools).with_fallbacks(\\n[RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\\nexception_key=\"error\"  # Specify that this fallback is for handling errors\\n)\\nThese functions ensure that any errors encountered during the tool’s execution are handled gracefully, providing helpful feedback to the user.\\nStep 4: Define the State and Assistant Class\\nIn this step, we’ll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the user’s input and tool output.\\nTo do this, we create a\\nState\\nclass using Python\\'s\\nTypedDict\\nto define the structure of the messages that will be passed around. The state will hold messages, including input from the user and output from the agent or tools.\\nclass State(TypedDict):\\nmessages: Annotated[list[AnyMessage], add_messages]\\nNext, we create the Assistant class, which is responsible for running the AI agent, interacting with the tools, and managing the flow of the conversation. The Assistant invokes the tools, ensures they return appropriate results, and handles any re-prompts or errors that may occur during execution. Its core functionality includes invoking the Runnable, which defines the process of calling the LLM and tools like\\ncompute_savings\\n, and then monitoring the results. If the agent fails to return a valid response or if a tool doesn\\'t provide meaningful data, the Assistant re-prompts the user or requests clarification. It continues to loop through the Runnable until a valid output is obtained, ensuring smooth execution and effective responses.\\nclass Assistant:\\ndef __init__(self, runnable: Runnable):\\n# Initialize with the runnable that defines the process for interacting with the tools\\nself.runnable = runnable\\ndef __call__(self, state: State):\\nwhile True:\\n# Invoke the runnable with the current state (messages and context)\\nresult = self.runnable.invoke(state)\\n# If the tool fails to return valid output, re-prompt the user to clarify or retry\\nif not result.tool_calls and (\\nnot result.content\\nor isinstance(result.content, list)\\nand not result.content[0].get(\"text\")\\n):\\n# Add a message to request a valid response\\nmessages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\\nstate = {**state, \"messages\": messages}\\nelse:\\n# Break the loop when valid output is obtained\\nbreak\\n# Return the final state after processing the runnable\\nreturn {\"messages\": result}\\nThis setup is essential for maintaining the flow of conversation and ensuring that the assistant responds appropriately based on the context.\\nStep 5: Set Up the LLM with AWS Bedrock\\nIn this step, we configure the Large Language Model (LLM) using AWS Bedrock, which will power the AI assistant’s language capabilities. AWS Bedrock allows us to access advanced LLMs such as Anthropic’s Claude. To interact with AWS services, you need to have your\\nAWS credentials configured\\n. This means you must either have your AWS credentials set in your environment (through the AWS CLI or environment variables) or use a credentials file that AWS SDKs can access. Without proper AWS configuration, the assistant won’t be able to connect to AWS services like Bedrock for running the LLM.\\ndef get_bedrock_client(region):\\nreturn boto3.client(\"bedrock-runtime\", region_name=region)\\ndef create_bedrock_llm(client):\\nreturn ChatBedrock(model_id=\\'anthropic.claude-3-sonnet-20240229-v1:0\\', client=client, model_kwargs={\\'temperature\\': 0}, region_name=\\'us-east-1\\')\\nllm = create_bedrock_llm(get_bedrock_client(region=\\'us-east-1\\'))\\nThis integration ensures that the assistant can effectively interpret and respond to user inputs.\\nStep 6: Define the Assistant’s Workflow\\nNow that we have set up the LLM and tools, the next step is to define the AI assistant’s workflow. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). The workflow is essentially the logic that governs how the assistant interacts with users, gathers information, and calls tools to provide results.\\nThe first part of the workflow involves\\ncreating a prompt template\\nthat defines how the assistant will communicate with the user. The prompt helps guide the AI assistant in determining what to ask the user, how to respond based on the input, and when to trigger tools like\\ncompute_savings\\n.\\nIn this case, the assistant needs to ask the user for their monthly electricity cost to calculate solar panel savings. Here’s how we define the conversation:\\nprimary_assistant_prompt = ChatPromptTemplate.from_messages(\\n[\\n(\\n\"system\",\\n\\'\\'\\'You are a helpful customer support assistant for Solar Panels Belgium.\\nYou should get the following information from them:\\n- monthly electricity cost\\nIf you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\\nAfter you are able to discern all the information, call the relevant tool.\\n\\'\\'\\',\\n),\\n(\"placeholder\", \"{messages}\"),\\n]\\n)\\nsystem\\nmessage\\n: This message acts as a guide for the AI agent, instructing it to ask the user for their monthly electricity cost and not make guesses if the information is unclear. The assistant will keep prompting the user until it gathers the required data.\\nplaceholder\\n: This placeholder allows the assistant to inject the messages from the conversation dynamically, creating an ongoing dialogue where the user’s input influences the next steps.\\nNext, we define the tools that the assistant will use during the interaction, with the primary tool being\\ncompute_savings\\n, which calculates potential savings based on the user\\'s monthly electricity cost. After specifying the tools in the list, we bind them to the assistant\\'s workflow using the\\nllm.bind_tools()\\nmethod. This step ensures that the AI assistant can access and trigger the tools as needed during the conversation, creating a seamless interaction between the user and the assistant.\\n# Define the tools the assistant will use\\npart_1_tools = [\\ncompute_savings\\n]\\n# Bind the tools to the assistant\\'s workflow\\npart_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)\\nStep 7: Build the Graph Structure\\nIn this step, we construct the\\ngraph structure\\nfor the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines\\nnodes\\nfor the core actions (like invoking the assistant and tool) and\\nedges\\nthat dictate the flow between these nodes.\\nAI agent designed to calculate potential energy savings for solar panels.\\nEach\\nnode\\nin LangGraph represents an operational step, such as interacting with the user or executing a tool. We define two key nodes for this AI assistant:\\nAssistant Node\\n: Manages the conversation flow, asking the user for their electricity cost and handling responses.\\nTool Node\\n: Executes the tool (e.g.,\\ncompute_savings\\n) to calculate the user’s solar panel savings.\\nbuilder = StateGraph(State)\\nbuilder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\\nbuilder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\\nEdges\\ndefine how the flow moves between nodes. Here, the assistant starts the conversation, then transitions to the tool once the required input is collected, and returns to the assistant after the tool’s execution.\\nbuilder.add_edge(START, \"assistant\")  # Start with the assistant\\nbuilder.add_conditional_edges(\"assistant\", tools_condition)  # Move to tools after input\\nbuilder.add_edge(\"tools\", \"assistant\")  # Return to assistant after tool execution\\nWe use\\nMemorySaver\\nto ensure the graph retains the conversation state across different steps. This allows the assistant to remember the user’s input, ensuring continuity in multi-step interactions.\\nmemory = MemorySaver()\\ngraph = builder.compile(checkpointer=memory)\\nStep 8: Running the Assistant\\nFinally, you can run the assistant by initiating the graph and starting the conversation.\\n# import shutil\\nimport uuid\\n# Let\\'s create an example conversation a user might have with the assistant\\ntutorial_questions = [\\n\\'hey\\',\\n\\'can you calculate my energy saving\\',\\n\"my montly cost is $100, what will i save\"\\n]\\nthread_id = str(uuid.uuid4())\\nconfig = {\\n\"configurable\": {\\n\"thread_id\": thread_id,\\n}\\n}\\n_printed = set()\\nfor question in tutorial_questions:\\nevents = graph.stream(\\n{\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\\n)\\nfor event in events:\\n_print_event(event, _printed)\\nConclusion\\nBy following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether you’re developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and robustness needed to bring your ideas to life.\\nInterested in visualizing and testing this agent using LangGraph Studio? Check out my recent article and YouTube video:\\nLangGraph Studio: Visualizing and Testing AI Agents with LangChain\\nA guide to using LangGraph Studio for visualizing, testing, and developing AI agents with LangChain. Includes setup and…\\nmedium.com\\nFollow me for more AI deep dives!\\nMedium\\n,\\nInstagram\\n,\\nYouTube\\nArtificial Intelligence\\nLlm\\nLangchain\\nLanggraph\\nAi Agent\\n--\\n--\\n13\\nFollow\\nWritten by\\nLore Van Oudenhove\\n546 Followers\\n·\\n20 Following\\nFollow\\nResponses (\\n13\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'From Basics to Advanced: Exploring LangGraph | by Mariya Mansurova | Towards Data Science\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nFrom Basics to Advanced: Exploring LangGraph\\nBuilding single- and multi-agent workflows with human-in-the-loop interactions\\nMariya Mansurova\\n·\\nFollow\\nPublished in\\nTowards Data Science\\n·\\n21 min read\\n·\\nAug 15, 2024\\n--\\n2\\nListen\\nShare\\nImage by DALL-E 3\\nLangChain\\nis one of the leading frameworks for building applications powered by Lardge Language Models. With the\\nLangChain Expression Language\\n(LCEL), defining and executing step-by-step action sequences — also known as chains — becomes much simpler. In more technical terms, LangChain allows us to create DAGs (directed acyclic graphs).\\nAs LLM applications, particularly LLM agents, have evolved, we’ve begun to use LLMs not just for execution but also as reasoning engines. This shift has introduced interactions that frequently involve repetition (cycles) and complex conditions. In such scenarios, LCEL is not sufficient, so LangChain implemented a new module —\\nLangGraph\\n.\\nLangGraph (as you might guess from the name) models all interactions as cyclical graphs. These graphs enable the development of advanced workflows and interactions with multiple loops and if-statements, making it a handy tool for creating both agent and multi-agent workflows.\\nIn this article, I will explore LangGraph’s key features and capabilities, including multi-agent applications. We’ll build a system that can answer different types of questions and dive into how to implement a human-in-the-loop setup.\\nIn\\nthe previous article\\n, we tried using CrewAI, another popular framework for multi-agent systems. LangGraph, however, takes a different approach. While CrewAI is a high-level framework with many predefined features and ready-to-use components, LangGraph operates at a lower level, offering extensive customization and control.\\nWith that introduction, let’s dive into the fundamental concepts of LangGraph.\\nLangGraph basics\\nLangGraph is part of the LangChain ecosystem, so we will continue using well-known concepts like prompt templates, tools, etc. However, LangGraph brings a bunch of\\nadditional concepts.\\nLet’s discuss them.\\nLangGraph is created to define cyclical graphs. Graphs consist of the following elements:\\nNodes represent actual actions and can be either LLMs, agents or functions. Also, a special END node marks the end of execution.\\nEdges connect nodes and determine the execution flow of your graph. There are basic edges that simply link one node to another and conditional edges that incorporate if-statements and additional logic.\\nAnother important concept is the state of the graph. The state serves as a foundational element for collaboration among the graph’s components. It represents a snapshot of the graph that any part — whether nodes or edges — can access and modify during execution to retrieve or update information.\\nAdditionally, the state plays a crucial role in persistence. It is automatically saved after each step, allowing you to pause and resume execution at any point. This feature supports the development of more complex applications, such as those requiring error correction or incorporating human-in-the-loop interactions.\\nSingle-agent workflow\\nBuilding agent from scratch\\nLet’s start simple and try using LangGraph for a basic use case — an agent with tools.\\nI will try to build similar applications to those we did with CrewAI in\\nthe previous article\\n. Then, we will be able to compare the two frameworks. For this example, let’s create an application that can automatically generate documentation based on the table in the database. It can save us quite a lot of time when creating documentation for our data sources.\\nAs usual, we will start by defining the tools for our agent. Since I will use the ClickHouse database in this example, I’ve defined a function to execute any query. You can use a different database if you prefer, as we won’t rely on any database-specific features.\\nCH_HOST = \\'http://localhost:8123\\' # default address\\nimport requests\\ndef get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):\\nr = requests.post(host, params = {\\'query\\': query},\\ntimeout = connection_timeout)\\nif r.status_code == 200:\\nreturn r.text\\nelse:\\nreturn \\'Database returned the following error:\\\\n\\' + r.text\\nIt’s crucial to make LLM tools reliable and error-prone. If a database returns an error, I provide this feedback to the LLM rather than throwing an exception and halting execution. Then, the LLM agent will have an opportunity to fix an error and call the function again.\\nLet’s define one tool named\\nexecute_sql\\n, which enables the execution of any SQL query. We use\\npydantic\\nto specify the tool’s structure, ensuring that the LLM agent has all the needed information to use the tool effectively.\\nfrom langchain_core.tools import tool\\nfrom pydantic.v1 import BaseModel, Field\\nfrom typing import Optional\\nclass SQLQuery(BaseModel):\\nquery: str = Field(description=\"SQL query to execute\")\\n@tool(args_schema = SQLQuery)\\ndef execute_sql(query: str) -> str:\\n\"\"\"Returns the result of SQL query execution\"\"\"\\nreturn get_clickhouse_data(query)\\nWe can print the parameters of the created tool to see what information is passed to LLM.\\nprint(f\\'\\'\\'\\nname: {execute_sql.name}\\ndescription: {execute_sql.description}\\narguments: {execute_sql.args}\\n\\'\\'\\')\\n# name: execute_sql\\n# description: Returns the result of SQL query execution\\n# arguments: {\\'query\\': {\\'title\\': \\'Query\\', \\'description\\':\\n#   \\'SQL query to execute\\', \\'type\\': \\'string\\'}}\\nEverything looks good. We’ve set up the necessary tool and can now move on to defining an LLM agent. As we discussed above, the cornerstone of the agent in LangGraph is its state, which enables the sharing of information between different parts of our graph.\\nOur current example is relatively straightforward. So, we will only need to store the history of messages. Let’s define the agent state.\\n# useful imports\\nfrom langgraph.graph import StateGraph, END\\nfrom typing import TypedDict, Annotated\\nimport operator\\nfrom langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\\n# defining agent state\\nclass AgentState(TypedDict):\\nmessages: Annotated[list[AnyMessage], operator.add]\\nWe’ve defined a single parameter in\\nAgentState\\n—\\nmessages\\n— which is a list of objects of the class\\nAnyMessage\\n. Additionally, we annotated it with\\noperator.add\\n(reducer). This annotation ensures that each time a node returns a message, it is appended to the existing list in the state. Without this operator, each new message would replace the previous value rather than being added to the list.\\nThe next step is to define the agent itself. Let’s start with\\n__init__\\nfunction. We will specify three arguments for the agent: model, list of tools and system prompt.\\nclass SQLAgent:\\n# initialising the object\\ndef __init__(self, model, tools, system_prompt = \"\"):\\nself.system_prompt = system_prompt\\n# initialising graph with a state\\ngraph = StateGraph(AgentState)\\n# adding nodes\\ngraph.add_node(\"llm\", self.call_llm)\\ngraph.add_node(\"function\", self.execute_function)\\ngraph.add_conditional_edges(\\n\"llm\",\\nself.exists_function_calling,\\n{True: \"function\", False: END}\\n)\\ngraph.add_edge(\"function\", \"llm\")\\n# setting starting point\\ngraph.set_entry_point(\"llm\")\\nself.graph = graph.compile()\\nself.tools = {t.name: t for t in tools}\\nself.model = model.bind_tools(tools)\\nIn the initialisation function, we’ve outlined the structure of our graph, which includes two nodes:\\nllm\\nand\\naction\\n. Nodes are actual actions, so we have functions associated with them. We will define functions a bit later.\\nAdditionally, we have one conditional edge that determines whether we need to execute the function or generate the final answer. For this edge, we need to specify the previous node (in our case,\\nllm\\n), a function that decides the next step, and mapping of the subsequent steps based on the function’s output (formatted as a dictionary). If\\nexists_function_calling\\nreturns True, we follow to the function node. Otherwise, execution will conclude at the special\\nEND\\nnode, which marks the end of the process.\\nWe’ve added an edge between\\nfunction\\nand\\nllm\\n. It just links these two steps and will be executed without any conditions.\\nWith the main structure defined, it’s time to create all the functions outlined above. The first one is\\ncall_llm\\n. This function will execute LLM and return the result.\\nThe agent state will be passed to the function automatically so we can use the saved system prompt and model from it.\\nclass SQLAgent:\\n<...>\\ndef call_llm(self, state: AgentState):\\nmessages = state[\\'messages\\']\\n# adding system prompt if it\\'s defined\\nif self.system_prompt:\\nmessages = [SystemMessage(content=self.system_prompt)] + messages\\n# calling LLM\\nmessage = self.model.invoke(messages)\\nreturn {\\'messages\\': [message]}\\nAs a result, our function returns a dictionary that will be used to update the agent state. Since we used\\noperator.add\\nas a reducer for our state, the returned message will be appended to the list of messages stored in the state.\\nThe next function we need is\\nexecute_function\\nwhich will run our tools. If the LLM agent decides to call a tool, we will see it in the\\nmessage.tool_calls\\nparameter.\\nclass SQLAgent:\\n<...>\\ndef execute_function(self, state: AgentState):\\ntool_calls = state[\\'messages\\'][-1].tool_calls\\nresults = []\\nfor tool in tool_calls:\\n# checking whether tool name is correct\\nif not t[\\'name\\'] in self.tools:\\n# returning error to the agent\\nresult = \"Error: There\\'s no such tool, please, try again\"\\nelse:\\n# getting result from the tool\\nresult = self.tools[t[\\'name\\']].invoke(t[\\'args\\'])\\nresults.append(\\nToolMessage(\\ntool_call_id=t[\\'id\\'],\\nname=t[\\'name\\'],\\ncontent=str(result)\\n)\\n)\\nreturn {\\'messages\\': results}\\nIn this function, we iterate over the tool calls returned by LLM and either invoke these tools or return the error message. In the end, our function returns the dictionary with a single key\\nmessages\\nthat will be used to update the graph state.\\nThere’s only one function left —the function for the conditional edge that defines whether we need to execute the tool or provide the final result. It’s pretty straightforward. We just need to check whether the last message contains any tool calls.\\nclass SQLAgent:\\n<...>\\ndef exists_function_calling(self, state: AgentState):\\nresult = state[\\'messages\\'][-1]\\nreturn len(result.tool_calls) > 0\\nIt’s time to create an agent and LLM model for it. I will use the new OpenAI GPT 4o mini model (\\ndoc\\n) since it’s cheaper and better performing than GPT 3.5.\\nimport os\\n# setting up credentioals\\nos.environ[\"OPENAI_MODEL_NAME\"]=\\'gpt-4o-mini\\'\\nos.environ[\"OPENAI_API_KEY\"] = \\'<your_api_key>\\'\\n# system prompt\\nprompt = \\'\\'\\'You are a senior expert in SQL and data analysis.\\nSo, you can help the team to gather needed data to power their decisions.\\nYou are very accurate and take into account all the nuances in data.\\nYour goal is to provide the detailed documentation for the table in database\\nthat will help users.\\'\\'\\'\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\ndoc_agent = SQLAgent(model, [execute_sql], system=prompt)\\nLangGraph provides us with quite a handy feature to visualise graphs. To use it, you need to install\\npygraphviz\\n.\\nIt’s a bit tricky for Mac with M1/M2 chips, so here is the lifehack for you (\\nsource\\n):\\n! brew install graphviz\\n! python3 -m pip install -U --no-cache-dir  \\\\\\n--config-settings=\"--global-option=build_ext\" \\\\\\n--config-settings=\"--global-option=-I$(brew --prefix graphviz)/include/\" \\\\\\n--config-settings=\"--global-option=-L$(brew --prefix graphviz)/lib/\" \\\\\\npygraphviz\\nAfter figuring out the installation, here’s our graph.\\nfrom IPython.display import Image\\nImage(doc_agent.graph.get_graph().draw_png())\\nAs you can see, our graph has cycles. Implementing something like this with LCEL would be quite challenging.\\nFinally, it’s time to execute our agent. We need to pass the initial set of messages with our questions as\\nHumanMessage\\n.\\nmessages = [HumanMessage(content=\"What info do we have in ecommerce_db.users table?\")]\\nresult = doc_agent.graph.invoke({\"messages\": messages})\\nIn the\\nresult\\nvariable, we can observe all the messages generated during execution. The process worked as expected:\\nThe agent decided to call the function with the query\\ndescribe ecommerce.db_users\\n.\\nLLM then processed the information from the tool and provided a user-friendly answer.\\nresult[\\'messages\\']\\n# [\\n#   HumanMessage(content=\\'What info do we have in ecommerce_db.users table?\\'),\\n#   AIMessage(content=\\'\\', tool_calls=[{\\'name\\': \\'execute_sql\\', \\'args\\': {\\'query\\': \\'DESCRIBE ecommerce_db.users;\\'}, \\'id\\': \\'call_qZbDU9Coa2tMjUARcX36h0ax\\', \\'type\\': \\'tool_call\\'}]),\\n#   ToolMessage(content=\\'user_id\\\\tUInt64\\\\t\\\\t\\\\t\\\\t\\\\t\\\\ncountry\\\\tString\\\\t\\\\t\\\\t\\\\t\\\\t\\\\nis_active\\\\tUInt8\\\\t\\\\t\\\\t\\\\t\\\\t\\\\nage\\\\tUInt64\\\\t\\\\t\\\\t\\\\t\\\\t\\\\n\\', name=\\'execute_sql\\', tool_call_id=\\'call_qZbDU9Coa2tMjUARcX36h0ax\\'),\\n#   AIMessage(content=\\'The `ecommerce_db.users` table contains the following columns: <...>\\')\\n# ]\\nHere’s the final result. It looks pretty decent.\\nprint(result[\\'messages\\'][-1].content)\\n# The `ecommerce_db.users` table contains the following columns:\\n# 1. **user_id**: `UInt64` - A unique identifier for each user.\\n# 2. **country**: `String` - The country where the user is located.\\n# 3. **is_active**: `UInt8` - Indicates whether the user is active (1) or inactive (0).\\n# 4. **age**: `UInt64` - The age of the user.\\nUsing prebuilt agents\\nWe’ve learned how to build an agent from scratch. However, we can leverage LangGraph\\'s built-in functionality for simpler tasks like this one.\\nWe can use a\\nprebuilt ReAct agent\\nto get a similar result: an agent that can work with tools.\\nfrom langgraph.prebuilt import create_react_agent\\nprebuilt_doc_agent = create_react_agent(model, [execute_sql],\\nstate_modifier = system_prompt)\\nIt is the same agent as we built previously. We will try it out in a second, but first, we need to understand two other important concepts: persistence and streaming.\\nPersistence and streaming\\nPersistence refers to the ability to maintain context across different interactions. It’s essential for agentic use cases when an application can get additional input from the user.\\nLangGraph automatically saves the state after each step, allowing you to pause or resume execution. This capability supports the implementation of advanced business logic, such as error recovery or human-in-the-loop interactions.\\nThe easiest way to add persistence is to use an in-memory SQLite database.\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nmemory = SqliteSaver.from_conn_string(\":memory:\")\\nFor the off-the-shelf agent, we can pass memory as an argument while creating an agent.\\nprebuilt_doc_agent = create_react_agent(model, [execute_sql],\\ncheckpointer=memory)\\nIf you’re working with a custom agent, you need to pass memory as a check pointer while compiling a graph.\\nclass SQLAgent:\\ndef __init__(self, model, tools, system_prompt = \"\"):\\n<...>\\nself.graph = graph.compile(checkpointer=memory)\\n<...>\\nLet’s execute the agent and explore another feature of LangGraph: streaming. With streaming, we can receive results from each step of execution as a separate event in a stream. This feature is crucial for production applications when multiple conversations (or threads) need to be processed simultaneously.\\nLangGraph supports not only event streaming but also token-level streaming. The only use case I have in mind for token streaming is to display the answers in real-time word by word (similar to ChatGPT implementation).\\nLet’s try using streaming with our new prebuilt agent. I will also use the\\npretty_print\\nfunction for messages to make the result more readable.\\n# defining thread\\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\\nmessages = [HumanMessage(content=\"What info do we have in ecommerce_db.users table?\")]\\nfor event in prebuilt_doc_agent.stream({\"messages\": messages}, thread):\\nfor v in event.values():\\nv[\\'messages\\'][-1].pretty_print()\\n# ================================== Ai Message ==================================\\n# Tool Calls:\\n#  execute_sql (call_YieWiChbFuOlxBg8G1jDJitR)\\n#  Call ID: call_YieWiChbFuOlxBg8G1jDJitR\\n#   Args:\\n#     query: SELECT * FROM ecommerce_db.users LIMIT 1;\\n# ================================= Tool Message =================================\\n# Name: execute_sql\\n# 1000001 United Kingdom 0 70\\n#\\n# ================================== Ai Message ==================================\\n#\\n# The `ecommerce_db.users` table contains at least the following information for users:\\n#\\n# - **User ID** (e.g., `1000001`)\\n# - **Country** (e.g., `United Kingdom`)\\n# - **Some numerical value** (e.g., `0`)\\n# - **Another numerical value** (e.g., `70`)\\n#\\n# The specific meaning of the numerical values and additional columns\\n# is not clear from the single row retrieved. Would you like more details\\n# or a broader query?\\nInterestingly, the agent wasn’t able to provide a good enough result. Since the agent didn’t look up the table schema, it struggled to guess all columns’ meanings. We can improve the result by using follow-up questions in the same thread.\\nfollowup_messages = [HumanMessage(content=\"I would like to know the column names and types. Maybe you could look it up in database using describe.\")]\\nfor event in prebuilt_doc_agent.stream({\"messages\": followup_messages}, thread):\\nfor v in event.values():\\nv[\\'messages\\'][-1].pretty_print()\\n# ================================== Ai Message ==================================\\n# Tool Calls:\\n#   execute_sql (call_sQKRWtG6aEB38rtOpZszxTVs)\\n#  Call ID: call_sQKRWtG6aEB38rtOpZszxTVs\\n#   Args:\\n#     query: DESCRIBE ecommerce_db.users;\\n# ================================= Tool Message =================================\\n# Name: execute_sql\\n#\\n# user_id UInt64\\n# country String\\n# is_active UInt8\\n# age UInt64\\n#\\n# ================================== Ai Message ==================================\\n#\\n# The `ecommerce_db.users` table has the following columns along with their data types:\\n#\\n# | Column Name | Data Type |\\n# |-------------|-----------|\\n# | user_id     | UInt64    |\\n# | country     | String    |\\n# | is_active   | UInt8     |\\n# | age         | UInt64    |\\n#\\n# If you need further information or assistance, feel free to ask!\\nThis time, we got the full answer from the agent. Since we provided the same thread, the agent was able to get the context from the previous discussion. That’s how persistence works.\\nLet’s try to change the thread and ask the same follow-up question.\\nnew_thread = {\"configurable\": {\"thread_id\": \"42\"}}\\nfollowup_messages = [HumanMessage(content=\"I would like to know the column names and types. Maybe you could look it up in database using describe.\")]\\nfor event in prebuilt_doc_agent.stream({\"messages\": followup_messages}, new_thread):\\nfor v in event.values():\\nv[\\'messages\\'][-1].pretty_print()\\n# ================================== Ai Message ==================================\\n# Tool Calls:\\n#   execute_sql (call_LrmsOGzzusaLEZLP9hGTBGgo)\\n#  Call ID: call_LrmsOGzzusaLEZLP9hGTBGgo\\n#   Args:\\n#     query: DESCRIBE your_table_name;\\n# ================================= Tool Message =================================\\n# Name: execute_sql\\n#\\n# Database returned the following error:\\n# Code: 60. DB::Exception: Table default.your_table_name does not exist. (UNKNOWN_TABLE) (version 23.12.1.414 (official build))\\n#\\n# ================================== Ai Message ==================================\\n#\\n# It seems that the table `your_table_name` does not exist in the database.\\n# Could you please provide the actual name of the table you want to describe?\\nIt was not surprising that the agent lacked the context needed to answer our question. Threads are designed to isolate different conversations, ensuring that each thread maintains its own context.\\nIn real-life applications, managing memory is essential. Conversations might become pretty lengthy, and at some point, it won’t be practical to pass the whole history to LLM every time. Therefore, it’s worth trimming or filtering messages. We won’t go deep into the specifics here, but you can find guidance on it in\\nthe LangGraph documentation\\n. Another option to compress the conversational history is using summarization (\\nexample\\n).\\nWe’ve learned how to build systems with single agents using LangGraph. The next step is to combine multiple agents in one application.\\nMulti-Agent Systems\\nAs an example of a multi-agent workflow, I would like to build an application that can handle questions from various domains. We will have a set of expert agents, each specializing in different types of questions, and a router agent that will find the best-suited expert to address each query. Such an application has numerous potential use cases: from automating customer support to answering questions from colleagues in internal chats.\\nFirst, we need to create the agent state — the information that will help agents to solve the question together. I will use the following fields:\\nquestion\\n— initial customer request;\\nquestion_type\\n— the category that defines which agent will be working on the request;\\nanswer\\n— the proposed answer to the question;\\nfeedback\\n— a field for future use that will gather some feedback.\\nclass MultiAgentState(TypedDict):\\nquestion: str\\nquestion_type: str\\nanswer: str\\nfeedback: str\\nI don’t use any reducers, so our state will store only the latest version of each field.\\nThen, let’s create a router node. It will be a simple LLM model that defines the category of question (database, LangChain or general questions).\\nquestion_category_prompt = \\'\\'\\'You are a senior specialist of analytical support. Your task is to classify the incoming questions.\\nDepending on your answer, question will be routed to the right team, so your task is crucial for our team.\\nThere are 3 possible question types:\\n- DATABASE - questions related to our database (tables or fields)\\n- LANGCHAIN- questions related to LangGraph or LangChain libraries\\n- GENERAL - general questions\\nReturn in the output only one word (DATABASE, LANGCHAIN or  GENERAL).\\n\\'\\'\\'\\ndef router_node(state: MultiAgentState):\\nmessages = [\\nSystemMessage(content=question_category_prompt),\\nHumanMessage(content=state[\\'question\\'])\\n]\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nresponse = model.invoke(messages)\\nreturn {\"question_type\": response.content}\\nNow that we have our first node — the router — let’s build a simple graph to test the workflow.\\nmemory = SqliteSaver.from_conn_string(\":memory:\")\\nbuilder = StateGraph(MultiAgentState)\\nbuilder.add_node(\"router\", router_node)\\nbuilder.set_entry_point(\"router\")\\nbuilder.add_edge(\\'router\\', END)\\ngraph = builder.compile(checkpointer=memory)\\nLet’s test our workflow with different types of questions to see how it performs in action. This will help us evaluate whether the router agent correctly assigns questions to the appropriate expert agents.\\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\\nfor s in graph.stream({\\n\\'question\\': \"Does LangChain support Ollama?\",\\n}, thread):\\nprint(s)\\n# {\\'router\\': {\\'question_type\\': \\'LANGCHAIN\\'}}\\nthread = {\"configurable\": {\"thread_id\": \"2\"}}\\nfor s in graph.stream({\\n\\'question\\': \"What info do we have in ecommerce_db.users table?\",\\n}, thread):\\nprint(s)\\n# {\\'router\\': {\\'question_type\\': \\'DATABASE\\'}}\\nthread = {\"configurable\": {\"thread_id\": \"3\"}}\\nfor s in graph.stream({\\n\\'question\\': \"How are you?\",\\n}, thread):\\nprint(s)\\n# {\\'router\\': {\\'question_type\\': \\'GENERAL\\'}}\\nIt’s working well. I recommend you build complex graphs incrementally and test each step independently. With such an approach, you can ensure that each iteration works expectedly and can save you a significant amount of debugging time.\\nNext, let’s create nodes for our expert agents. We will use the ReAct agent with the SQL tool we previously built as the database agent.\\n# database expert\\nsql_expert_system_prompt = \\'\\'\\'\\nYou are an expert in SQL, so you can help the team\\nto gather needed data to power their decisions.\\nYou are very accurate and take into account all the nuances in data.\\nYou use SQL to get the data before answering the question.\\n\\'\\'\\'\\ndef sql_expert_node(state: MultiAgentState):\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nsql_agent = create_react_agent(model, [execute_sql],\\nstate_modifier = sql_expert_system_prompt)\\nmessages = [HumanMessage(content=state[\\'question\\'])]\\nresult = sql_agent.invoke({\"messages\": messages})\\nreturn {\\'answer\\': result[\\'messages\\'][-1].content}\\nFor LangChain-related questions, we will use the ReAct agent. To enable the agent to answer questions about the library, we will equip it with a search engine tool. I chose\\nTavily\\nfor this purpose as it provides the search results optimised for LLM applications.\\nIf you don’t have an account, you can register to use Tavily for free (up to 1K requests per month). To get started, you will need to specify the Tavily API key in an environment variable.\\n# search expert\\nfrom langchain_community.tools.tavily_search import TavilySearchResults\\nos.environ[\"TAVILY_API_KEY\"] = \\'tvly-...\\'\\ntavily_tool = TavilySearchResults(max_results=5)\\nsearch_expert_system_prompt = \\'\\'\\'\\nYou are an expert in LangChain and other technologies.\\nYour goal is to answer questions based on results provided by search.\\nYou don\\'t add anything yourself and provide only information baked by other sources.\\n\\'\\'\\'\\ndef search_expert_node(state: MultiAgentState):\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nsql_agent = create_react_agent(model, [tavily_tool],\\nstate_modifier = search_expert_system_prompt)\\nmessages = [HumanMessage(content=state[\\'question\\'])]\\nresult = sql_agent.invoke({\"messages\": messages})\\nreturn {\\'answer\\': result[\\'messages\\'][-1].content}\\nFor general questions, we will leverage a simple LLM model without specific tools.\\n# general model\\ngeneral_prompt = \\'\\'\\'You\\'re a friendly assistant and your goal is to answer general questions.\\nPlease, don\\'t provide any unchecked information and just tell that you don\\'t know if you don\\'t have enough info.\\n\\'\\'\\'\\ndef general_assistant_node(state: MultiAgentState):\\nmessages = [\\nSystemMessage(content=general_prompt),\\nHumanMessage(content=state[\\'question\\'])\\n]\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nresponse = model.invoke(messages)\\nreturn {\"answer\": response.content}\\nThe last missing bit is a conditional function for routing. This will be quite straightforward—we just need to propagate the question type from the state defined by the router node.\\ndef route_question(state: MultiAgentState):\\nreturn state[\\'question_type\\']\\nNow, it’s time to create our graph.\\nbuilder = StateGraph(MultiAgentState)\\nbuilder.add_node(\"router\", router_node)\\nbuilder.add_node(\\'database_expert\\', sql_expert_node)\\nbuilder.add_node(\\'langchain_expert\\', search_expert_node)\\nbuilder.add_node(\\'general_assistant\\', general_assistant_node)\\nbuilder.add_conditional_edges(\\n\"router\",\\nroute_question,\\n{\\'DATABASE\\': \\'database_expert\\',\\n\\'LANGCHAIN\\': \\'langchain_expert\\',\\n\\'GENERAL\\': \\'general_assistant\\'}\\n)\\nbuilder.set_entry_point(\"router\")\\nbuilder.add_edge(\\'database_expert\\', END)\\nbuilder.add_edge(\\'langchain_expert\\', END)\\nbuilder.add_edge(\\'general_assistant\\', END)\\ngraph = builder.compile(checkpointer=memory)\\nNow, we can test the setup on a couple of questions to see how well it performs.\\nthread = {\"configurable\": {\"thread_id\": \"2\"}}\\nresults = []\\nfor s in graph.stream({\\n\\'question\\': \"What info do we have in ecommerce_db.users table?\",\\n}, thread):\\nprint(s)\\nresults.append(s)\\nprint(results[-1][\\'database_expert\\'][\\'answer\\'])\\n# The `ecommerce_db.users` table contains the following columns:\\n# 1. **User ID**: A unique identifier for each user.\\n# 2. **Country**: The country where the user is located.\\n# 3. **Is Active**: A flag indicating whether the user is active (1 for active, 0 for inactive).\\n# 4. **Age**: The age of the user.\\n# Here are some sample entries from the table:\\n#\\n# | User ID | Country        | Is Active | Age |\\n# |---------|----------------|-----------|-----|\\n# | 1000001 | United Kingdom  | 0         | 70  |\\n# | 1000002 | France         | 1         | 87  |\\n# | 1000003 | France         | 1         | 88  |\\n# | 1000004 | Germany        | 1         | 25  |\\n# | 1000005 | Germany        | 1         | 48  |\\n#\\n# This gives an overview of the user data available in the table.\\nGood job! It gives a relevant result for the database-related question. Let’s try asking about LangChain.\\nthread = {\"configurable\": {\"thread_id\": \"42\"}}\\nresults = []\\nfor s in graph.stream({\\n\\'question\\': \"Does LangChain support Ollama?\",\\n}, thread):\\nprint(s)\\nresults.append(s)\\nprint(results[-1][\\'langchain_expert\\'][\\'answer\\'])\\n# Yes, LangChain supports Ollama. Ollama allows you to run open-source\\n# large language models, such as Llama 2, locally, and LangChain provides\\n# a flexible framework for integrating these models into applications.\\n# You can interact with models run by Ollama using LangChain, and there are\\n# specific wrappers and tools available for this integration.\\n#\\n# For more detailed information, you can visit the following resources:\\n# - [LangChain and Ollama Integration](https://js.langchain.com/v0.1/docs/integrations/llms/ollama/)\\n# - [ChatOllama Documentation](https://js.langchain.com/v0.2/docs/integrations/chat/ollama/)\\n# - [Medium Article on Ollama and LangChain](https://medium.com/@abonia/ollama-and-langchain-run-llms-locally-900931914a46)\\nFantastic! Everything is working well, and it’s clear that Tavily\\'s search is effective for LLM applications.\\nAdding human-in-the-loop interactions\\nWe’ve done an excellent job creating a tool to answer questions. However, in many cases, it’s beneficial to keep a human in the loop to approve proposed actions or provide additional feedback. Let’s add a step where we can collect feedback from a human before returning the final result to the user.\\nThe simplest approach is to add two additional nodes:\\nA\\nhuman\\nnode to gather feedback,\\nAn\\neditor\\nnode to revisit the answer, taking into account the feedback.\\nLet’s create these nodes:\\nHuman node:\\nThis will be a dummy node, and it won’t perform any actions.\\nEditor node:\\nThis will be an LLM model that receives all the relevant information (customer question, draft answer and provided feedback) and revises the final answer.\\ndef human_feedback_node(state: MultiAgentState):\\npass\\neditor_prompt = \\'\\'\\'You\\'re an editor and your goal is to provide the final answer to the customer, taking into account the feedback.\\nYou don\\'t add any information on your own. You use friendly and professional tone.\\nIn the output please provide the final answer to the customer without additional comments.\\nHere\\'s all the information you need.\\nQuestion from customer:\\n----\\n{question}\\n----\\nDraft answer:\\n----\\n{answer}\\n----\\nFeedback:\\n----\\n{feedback}\\n----\\n\\'\\'\\'\\ndef editor_node(state: MultiAgentState):\\nmessages = [\\nSystemMessage(content=editor_prompt.format(question = state[\\'question\\'], answer = state[\\'answer\\'], feedback = state[\\'feedback\\']))\\n]\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nresponse = model.invoke(messages)\\nreturn {\"answer\": response.content}\\nLet’s add these nodes to our graph. Additionally, we need to introduce an interruption before the human node to ensure that the process pauses for human feedback.\\nbuilder = StateGraph(MultiAgentState)\\nbuilder.add_node(\"router\", router_node)\\nbuilder.add_node(\\'database_expert\\', sql_expert_node)\\nbuilder.add_node(\\'langchain_expert\\', search_expert_node)\\nbuilder.add_node(\\'general_assistant\\', general_assistant_node)\\nbuilder.add_node(\\'human\\', human_feedback_node)\\nbuilder.add_node(\\'editor\\', editor_node)\\nbuilder.add_conditional_edges(\\n\"router\",\\nroute_question,\\n{\\'DATABASE\\': \\'database_expert\\',\\n\\'LANGCHAIN\\': \\'langchain_expert\\',\\n\\'GENERAL\\': \\'general_assistant\\'}\\n)\\nbuilder.set_entry_point(\"router\")\\nbuilder.add_edge(\\'database_expert\\', \\'human\\')\\nbuilder.add_edge(\\'langchain_expert\\', \\'human\\')\\nbuilder.add_edge(\\'general_assistant\\', \\'human\\')\\nbuilder.add_edge(\\'human\\', \\'editor\\')\\nbuilder.add_edge(\\'editor\\', END)\\ngraph = builder.compile(checkpointer=memory, interrupt_before = [\\'human\\'])\\nNow, when we run the graph, the execution will be stopped before the human node.\\nthread = {\"configurable\": {\"thread_id\": \"2\"}}\\nfor event in graph.stream({\\n\\'question\\': \"What are the types of fields in ecommerce_db.users table?\",\\n}, thread):\\nprint(event)\\n# {\\'question_type\\': \\'DATABASE\\', \\'question\\': \\'What are the types of fields in ecommerce_db.users table?\\'}\\n# {\\'router\\': {\\'question_type\\': \\'DATABASE\\'}}\\n# {\\'database_expert\\': {\\'answer\\': \\'The `ecommerce_db.users` table has the following fields:\\\\n\\\\n1. **user_id**: UInt64\\\\n2. **country**: String\\\\n3. **is_active**: UInt8\\\\n4. **age**: UInt64\\'}}\\nLet’s get the customer input and update the state with the feedback.\\nuser_input = input(\"Do I need to change anything in the answer?\")\\n# Do I need to change anything in the answer?\\n# It looks wonderful. Could you only make it a bit friendlier please?\\ngraph.update_state(thread, {\"feedback\": user_input}, as_node=\"human\")\\nWe can check the state to confirm that the feedback has been populated and that the next node in the sequence is\\neditor\\n.\\nprint(graph.get_state(thread).values[\\'feedback\\'])\\n# It looks wonderful. Could you only make it a bit friendlier please?\\nprint(graph.get_state(thread).next)\\n# (\\'editor\\',)\\nWe can just continue the execution. Passing\\nNone\\nas input will resume the process from the point where it was paused.\\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\\nprint(event)\\nprint(event[\\'answer\\'])\\n# Hello! The `ecommerce_db.users` table has the following fields:\\n# 1. **user_id**: UInt64\\n# 2. **country**: String\\n# 3. **is_active**: UInt8\\n# 4. **age**: UInt64\\n# Have a nice day!\\nThe editor took our feedback into account and added some polite words to our final message. That’s a fantastic result!\\nWe can implement human-in-the-loop interactions in a more agentic way by equipping our editor with the\\nHuman\\ntool.\\nLet’s adjust our editor. I’ve slightly changed the prompt and added the tool to the agent.\\nfrom langchain_community.tools import HumanInputRun\\nhuman_tool = HumanInputRun()\\neditor_agent_prompt = \\'\\'\\'You\\'re an editor and your goal is to provide the final answer to the customer, taking into the initial question.\\nIf you need any clarifications or need feedback, please, use human. Always reach out to human to get the feedback before final answer.\\nYou don\\'t add any information on your own. You use friendly and professional tone.\\nIn the output please provide the final answer to the customer without additional comments.\\nHere\\'s all the information you need.\\nQuestion from customer:\\n----\\n{question}\\n----\\nDraft answer:\\n----\\n{answer}\\n----\\n\\'\\'\\'\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\neditor_agent = create_react_agent(model, [human_tool])\\nmessages = [SystemMessage(content=editor_agent_prompt.format(question = state[\\'question\\'], answer = state[\\'answer\\']))]\\neditor_result = editor_agent.invoke({\"messages\": messages})\\n# Is the draft answer complete and accurate for the customer\\'s question about the types of fields in the ecommerce_db.users table?\\n# Yes, but could you please make it friendlier.\\nprint(editor_result[\\'messages\\'][-1].content)\\n# The `ecommerce_db.users` table has the following fields:\\n# 1. **user_id**: UInt64\\n# 2. **country**: String\\n# 3. **is_active**: UInt8\\n# 4. **age**: UInt64\\n#\\n# If you have any more questions, feel free to ask!\\nSo, the editor reached out to the human with the question, “Is the draft answer complete and accurate for the customer’s question about the types of fields in the ecommerce_db.users table?”. After receiving feedback, the editor refined the answer to make it more user-friendly.\\nLet’s update our main graph to incorporate the new agent instead of using the two separate nodes. With this approach, we don’t need interruptions any more.\\ndef editor_agent_node(state: MultiAgentState):\\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\\neditor_agent = create_react_agent(model, [human_tool])\\nmessages = [SystemMessage(content=editor_agent_prompt.format(question = state[\\'question\\'], answer = state[\\'answer\\']))]\\nresult = editor_agent.invoke({\"messages\": messages})\\nreturn {\\'answer\\': result[\\'messages\\'][-1].content}\\nbuilder = StateGraph(MultiAgentState)\\nbuilder.add_node(\"router\", router_node)\\nbuilder.add_node(\\'database_expert\\', sql_expert_node)\\nbuilder.add_node(\\'langchain_expert\\', search_expert_node)\\nbuilder.add_node(\\'general_assistant\\', general_assistant_node)\\nbuilder.add_node(\\'editor\\', editor_agent_node)\\nbuilder.add_conditional_edges(\\n\"router\",\\nroute_question,\\n{\\'DATABASE\\': \\'database_expert\\',\\n\\'LANGCHAIN\\': \\'langchain_expert\\',\\n\\'GENERAL\\': \\'general_assistant\\'}\\n)\\nbuilder.set_entry_point(\"router\")\\nbuilder.add_edge(\\'database_expert\\', \\'editor\\')\\nbuilder.add_edge(\\'langchain_expert\\', \\'editor\\')\\nbuilder.add_edge(\\'general_assistant\\', \\'editor\\')\\nbuilder.add_edge(\\'editor\\', END)\\ngraph = builder.compile(checkpointer=memory)\\nthread = {\"configurable\": {\"thread_id\": \"42\"}}\\nresults = []\\nfor event in graph.stream({\\n\\'question\\': \"What are the types of fields in ecommerce_db.users table?\",\\n}, thread):\\nprint(event)\\nresults.append(event)\\nThis graph will work similarly to the previous one. I personally prefer this approach since it leverages tools, making the solution more agile. For example, agents can reach out to humans multiple times and refine questions as needed.\\nThat’s it. We’ve built a multi-agent system that can answer questions from different domains and take into account human feedback.\\nYou can find the complete code on\\nGitHub\\n.\\nSummary\\nIn this article, we’ve explored the LangGraph library and its application for building single and multi-agent workflows. We’ve examined a range of its capabilities, and now it\\'s time to summarise its strengths and weaknesses. Also, it will be useful to compare LangGraph with CrewAI, which we discussed in\\nmy previous article\\n.\\nOverall, I find LangGraph quite a powerful framework for building complex LLM applications:\\nLangGraph is a low-level framework that offers extensive customisation options, allowing you to build precisely what you need.\\nSince LangGraph is built on top of LangChain, it’s seamlessly integrated into its ecosystem, making it easy to leverage existing tools and components.\\nHowever, there are areas where LangGrpah could be improved:\\nThe agility of LangGraph comes with a higher entry barrier. While you can understand the concepts of CrewAI within 15–30 minutes, it takes some time to get comfortable and up to speed with LangGraph.\\nLangGraph provides you with a higher level of control, but it misses some cool prebuilt features of CrewAI, such as\\ncollaboration\\nor ready-to-use\\nRAG\\ntools.\\nLangGraph doesn’t enforce best practices like CrewAI does (for example, role-playing or guardrails). So it can lead to poorer results.\\nI would say that CrewAI is a better framework for newbies and common use cases because it helps you get good results quickly and provides guidance to prevent mistakes.\\nIf you want to build an advanced application and need more control, LangGraph is the way to go. Keep in mind that you’ll need to invest time in learning LangGraph and should be fully responsible for the final solution, as the framework won’t provide guidance to help you avoid common mistakes.\\nThank you a lot for reading this article. I hope this article was insightful for you. If you have any follow-up questions or comments, please leave them in the comments section.\\nReference\\nThis article is inspired by the\\n“AI Agents in LangGraph”\\nshort course from DeepLearning.AI.\\nLlm\\nData Science\\nArtificial Intelligence\\nEditors Pick\\nDeep Dives\\n--\\n--\\n2\\nFollow\\nPublished in\\nTowards Data Science\\n775K Followers\\n·\\nLast published\\njust now\\nYour home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\\nFollow\\nFollow\\nWritten by\\nMariya Mansurova\\n12.4K Followers\\n·\\n122 Following\\nData & Product Analytics Lead at Wise | ClickHouse Evangelist\\nFollow\\nResponses (\\n2\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'Introduction to LangGraph: Core Concepts and Basic Components - DEV Community\\nSkip to content\\nNavigation menu\\nSearch\\nPowered by\\nSearch\\nAlgolia\\nSearch\\nLog in\\nCreate account\\nDEV Community\\nClose\\nAdd reaction\\nLike\\nUnicorn\\nExploding Head\\nRaised Hands\\nFire\\nJump to Comments\\nSave\\nBoost\\nMore...\\nCopy link\\nCopy link\\nCopied to Clipboard\\nShare to X\\nShare to LinkedIn\\nShare to Facebook\\nShare to Mastodon\\nReport Abuse\\nJames Li\\nPosted on\\nNov 14\\nIntroduction to LangGraph: Core Concepts and Basic Components\\nLangGraph Advanced Tutorial (9 Part Series)\\n1\\nTwo Basic Streaming Response Techniques of LangGraph\\n2\\nAdvanced Features of LangGraph: Summary and Considerations\\n...\\n5 more parts...\\n3\\nBuilding Complex AI Workflows with LangGraph: A Detailed Explanation of Subgraph Architecture\\n4\\nCheckpoints and Human-Computer Interaction in LangGraph\\n5\\nAdvanced Techniques in LangGraph: Tips for Using Message Deletion in Graph Structure Applications\\n6\\nAdvanced LangGraph: Building Intelligent Agents with ReACT Architecture\\n7\\nAdvanced LangGraph: Implementing Conditional Edges and Tool-Calling Agents\\n8\\nIntroduction to LangGraph: Core Concepts and Basic Components\\n9\\nAnalysis of Limitations of LCEL and AgentExecutor\\nIn the previous article, we discussed the limitations of LCEL and AgentExecutor. Today, we introduce a powerful solution—LangGraph, which addresses these issues through the concepts of graphs and state machines.\\nIntroduction to LangGraph\\nLangGraph is a new member of the LangChain ecosystem, providing a graph-based framework for building complex LLM applications. By organizing application logic into directed graphs, LangGraph makes constructing complex conversational flows more intuitive and flexible.\\nKey Features\\nLooping and Branching Capabilities\\n: Supports conditional statements and loop structures, allowing dynamic execution paths based on state.\\nState Persistence\\n: Automatically saves and manages state, supporting pause and resume for long-running conversations.\\nHuman-Machine Interaction Support\\n: Allows inserting human review during execution, supporting state editing and modification with flexible interaction control mechanisms.\\nStreaming Processing\\n: Supports streaming output and real-time feedback on execution status to enhance user experience.\\nSeamless Integration with LangChain\\n: Reuses existing LangChain components, supports LCEL expressions, and offers rich tool and model support.\\nCore Concepts\\n1. State\\nState is the foundation of LangGraph applications and can be a simple dictionary or a Pydantic model. It contains all the information needed during application runtime:\\nfrom\\ntyping\\nimport\\nList\\n,\\nDict\\nfrom\\npydantic\\nimport\\nBaseModel\\nclass\\nChatState\\n(\\nBaseModel\\n):\\nmessages\\n:\\nList\\n[\\nDict\\n[\\nstr\\n,\\nstr\\n]]\\n=\\n[]\\ncurrent_input\\n:\\nstr\\n=\\n\"\"\\ntools_output\\n:\\nDict\\n[\\nstr\\n,\\nstr\\n]\\n=\\n{}\\nfinal_response\\n:\\nstr\\n=\\n\"\"\\nEnter fullscreen mode\\nExit fullscreen mode\\n2. Node\\nNodes are typically Python functions used to process state and return updated state:\\nasync\\ndef\\nprocess_input\\n(\\nstate\\n:\\nChatState\\n)\\n->\\nChatState\\n:\\n# Process user input\\nmessages\\n=\\nstate\\n.\\nmessages\\n+\\n[{\\n\"\\nrole\\n\"\\n:\\n\"\\nuser\\n\"\\n,\\n\"\\ncontent\\n\"\\n:\\nstate\\n.\\ncurrent_input\\n}]\\nreturn\\nChatState\\n(\\nmessages\\n=\\nmessages\\n,\\ncurrent_input\\n=\\nstate\\n.\\ncurrent_input\\n,\\ntools_output\\n=\\nstate\\n.\\ntools_output\\n)\\nasync\\ndef\\ngenerate_response\\n(\\nstate\\n:\\nChatState\\n)\\n->\\nChatState\\n:\\n# Generate response using LLM\\nresponse\\n=\\nawait\\nllm\\n.\\nainvoke\\n(\\nstate\\n.\\nmessages\\n)\\nmessages\\n=\\nstate\\n.\\nmessages\\n+\\n[{\\n\"\\nrole\\n\"\\n:\\n\"\\nassistant\\n\"\\n,\\n\"\\ncontent\\n\"\\n:\\nresponse\\n}]\\nreturn\\nChatState\\n(\\nmessages\\n=\\nmessages\\n,\\ncurrent_input\\n=\\nstate\\n.\\ncurrent_input\\n,\\ntools_output\\n=\\nstate\\n.\\ntools_output\\n,\\nfinal_response\\n=\\nresponse\\n)\\nEnter fullscreen mode\\nExit fullscreen mode\\n3. Edge\\nEdges define the connections and routing logic between nodes:\\nfrom\\nlanggraph.graph\\nimport\\nStateGraph\\n,\\nEND\\n# Create graph structure\\nworkflow\\n=\\nStateGraph\\n(\\nChatState\\n)\\n# Add nodes\\nworkflow\\n.\\nadd_node\\n(\\n\"\\nprocess_input\\n\"\\n,\\nprocess_input\\n)\\nworkflow\\n.\\nadd_node\\n(\\n\"\\ngenerate_response\\n\"\\n,\\ngenerate_response\\n)\\n# Define edges and routing logic\\nworkflow\\n.\\nadd_edge\\n(\\n\"\\nprocess_input\\n\"\\n,\\n\"\\ngenerate_response\\n\"\\n)\\nworkflow\\n.\\nadd_edge\\n(\\n\"\\ngenerate_response\\n\"\\n,\\nEND\\n)\\nEnter fullscreen mode\\nExit fullscreen mode\\nPractical Example: Simple Chatbot\\nLet\\'s demonstrate the basic usage of LangGraph with a simple chatbot example:\\nfrom\\ntyping\\nimport\\nList\\n,\\nDict\\nfrom\\npydantic\\nimport\\nBaseModel\\nfrom\\nlanggraph.graph\\nimport\\nStateGraph\\n,\\nEND\\nfrom\\nlangchain_core.language_models\\nimport\\nChatOpenAI\\n# 1. Define state\\nclass\\nChatState\\n(\\nBaseModel\\n):\\nmessages\\n:\\nList\\n[\\nDict\\n[\\nstr\\n,\\nstr\\n]]\\n=\\n[]\\ncurrent_input\\n:\\nstr\\n=\\n\"\"\\nshould_continue\\n:\\nbool\\n=\\nTrue\\n# 2. Define node functions\\nasync\\ndef\\nprocess_user_input\\n(\\nstate\\n:\\nChatState\\n)\\n->\\nChatState\\n:\\n\"\"\"\\nProcess user input\\n\"\"\"\\nmessages\\n=\\nstate\\n.\\nmessages\\n+\\n[{\\n\"\\nrole\\n\"\\n:\\n\"\\nuser\\n\"\\n,\\n\"\\ncontent\\n\"\\n:\\nstate\\n.\\ncurrent_input\\n}]\\nreturn\\nChatState\\n(\\nmessages\\n=\\nmessages\\n,\\ncurrent_input\\n=\\nstate\\n.\\ncurrent_input\\n,\\nshould_continue\\n=\\nTrue\\n)\\nasync\\ndef\\ngenerate_ai_response\\n(\\nstate\\n:\\nChatState\\n)\\n->\\nChatState\\n:\\n\"\"\"\\nGenerate AI response\\n\"\"\"\\nllm\\n=\\nChatOpenAI\\n(\\ntemperature\\n=\\n0.7\\n)\\nresponse\\n=\\nawait\\nllm\\n.\\nainvoke\\n(\\nstate\\n.\\nmessages\\n)\\nmessages\\n=\\nstate\\n.\\nmessages\\n+\\n[{\\n\"\\nrole\\n\"\\n:\\n\"\\nassistant\\n\"\\n,\\n\"\\ncontent\\n\"\\n:\\nresponse\\n}]\\nreturn\\nChatState\\n(\\nmessages\\n=\\nmessages\\n,\\ncurrent_input\\n=\\nstate\\n.\\ncurrent_input\\n,\\nshould_continue\\n=\\nTrue\\n)\\ndef\\nshould_continue\\n(\\nstate\\n:\\nChatState\\n)\\n->\\nstr\\n:\\n\"\"\"\\nDecide whether to continue the conversation\\n\"\"\"\\nif\\n\"\\ngoodbye\\n\"\\nin\\nstate\\n.\\ncurrent_input\\n.\\nlower\\n():\\nreturn\\n\"\\nend\\n\"\\nreturn\\n\"\\ncontinue\\n\"\\n# 3. Build the graph\\nworkflow\\n=\\nStateGraph\\n(\\nChatState\\n)\\n# Add nodes\\nworkflow\\n.\\nadd_node\\n(\\n\"\\nprocess_input\\n\"\\n,\\nprocess_user_input\\n)\\nworkflow\\n.\\nadd_node\\n(\\n\"\\ngenerate_response\\n\"\\n,\\ngenerate_ai_response\\n)\\n# Add edges\\nworkflow\\n.\\nadd_edge\\n(\\n\"\\nprocess_input\\n\"\\n,\\n\"\\ngenerate_response\\n\"\\n)\\nworkflow\\n.\\nadd_conditional_edges\\n(\\n\"\\ngenerate_response\\n\"\\n,\\nshould_continue\\n,\\n{\\n\"\\ncontinue\\n\"\\n:\\n\"\\nprocess_input\\n\"\\n,\\n\"\\nend\\n\"\\n:\\nEND\\n})\\n# 4. Compile the graph\\napp\\n=\\nworkflow\\n.\\ncompile\\n()\\n# 5. Run the conversation\\nasync\\ndef\\nchat\\n():\\nstate\\n=\\nChatState\\n()\\nwhile\\nTrue\\n:\\nuser_input\\n=\\ninput\\n(\\n\"\\nYou:\\n\"\\n)\\nstate\\n.\\ncurrent_input\\n=\\nuser_input\\nstate\\n=\\nawait\\napp\\n.\\nainvoke\\n(\\nstate\\n)\\nprint\\n(\\n\"\\nBot:\\n\"\\n,\\nstate\\n.\\nmessages\\n[\\n-\\n1\\n][\\n\"\\ncontent\\n\"\\n])\\nif\\nnot\\nstate\\n.\\nshould_continue\\n:\\nbreak\\n# Run chat\\nimport\\nasyncio\\nasyncio\\n.\\nrun\\n(\\nchat\\n())\\nEnter fullscreen mode\\nExit fullscreen mode\\nThis example demonstrates the basic usage of LangGraph:\\nDefine state model\\nCreate processing nodes\\nBuild the graph structure\\nDefine routing logic\\nCompile and run\\nBest Practices\\nWhen using LangGraph, here are some best practices to keep in mind:\\nState Design\\n: Keep the state model simple and clear, only including necessary information. Use type hints to increase code readability.\\nNode Functions\\n: Maintain single responsibility, handle exceptions, and return new state objects instead of modifying existing state.\\nEdge Design\\n: Use clear conditional logic, avoid complex cyclic dependencies, and consider all possible paths.\\nError Handling\\n: Add error handling at critical nodes, provide fallback mechanisms, and log detailed error information.\\nConclusion\\nLangGraph simplifies the development of complex LLM applications by providing intuitive graphical structures and state management mechanisms. It not only addresses the limitations of LCEL and AgentExecutor but also offers more powerful features and a better development experience.\\nIn the next article, we will delve into the advanced features of LangGraph, including the advanced usage of conditional edges and how to implement complex tool-calling agents. We will showcase the powerful capabilities of LangGraph in real-world applications through more complex examples.\\nLangGraph Advanced Tutorial (9 Part Series)\\n1\\nTwo Basic Streaming Response Techniques of LangGraph\\n2\\nAdvanced Features of LangGraph: Summary and Considerations\\n...\\n5 more parts...\\n3\\nBuilding Complex AI Workflows with LangGraph: A Detailed Explanation of Subgraph Architecture\\n4\\nCheckpoints and Human-Computer Interaction in LangGraph\\n5\\nAdvanced Techniques in LangGraph: Tips for Using Message Deletion in Graph Structure Applications\\n6\\nAdvanced LangGraph: Building Intelligent Agents with ReACT Architecture\\n7\\nAdvanced LangGraph: Implementing Conditional Edges and Tool-Calling Agents\\n8\\nIntroduction to LangGraph: Core Concepts and Basic Components\\n9\\nAnalysis of Limitations of LCEL and AgentExecutor\\nTop comments\\n(0)\\nSubscribe\\nPersonal\\nTrusted User\\nCreate template\\nTemplates let you quickly answer FAQs or store snippets for re-use.\\nSubmit\\nPreview\\nDismiss\\nCode of Conduct\\n•\\nReport abuse\\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\'s\\npermalink\\n.\\nHide child comments as well\\nConfirm\\nFor further actions, you may consider blocking this person and/or\\nreporting abuse\\nRead next\\nThe 12 Easiest Programming Languages to Learn\\nCheremi Checo Domínguez -\\nDec 13\\nTop Websites Like OnlyFans: Best Alternatives for Creators\\nErasa.ai -\\nDec 13\\nTotal Noob Running Script & Source Code\\nJami -\\nDec 13\\nHow to Know My Proxy Server Address in Simple Steps\\nkerr -\\nDec 13\\nJames Li\\nFollow\\n🤖 Senior LLM Engineer | RAG & AI Agents | GenAI Solutions\\nLocation\\nhangzhou, China\\nJoined\\nNov 12, 2024\\nMore from\\nJames Li\\nBuilding a Medical Literature Assistant: RAG System Practice Based on LangChain\\n#\\nllm\\n#\\nai\\n#\\nlangchain\\n#\\nrag\\nBuild an enterprise-level financial data analysis assistant: multi-source data RAG system practice based on LangChain\\n#\\nllm\\n#\\nai\\n#\\npython\\n#\\nmachinelearning\\nEnterprise-Level Deployment and Optimization of LLM Applications: A Production Practice Guide Based on LangChain\\n#\\nlangchain\\n#\\narchitecture\\n#\\nllm\\n#\\nproductivity\\nThank you to our Diamond Sponsor\\nNeon\\nfor supporting our community.\\nDEV Community\\n— A constructive and inclusive social network for software developers. With you every step of your journey.\\nHome\\nDEV++\\nPodcasts\\nVideos\\nTags\\nDEV Help\\nForem Shop\\nAdvertise on DEV\\nDEV Challenges\\nDEV Showcase\\nAbout\\nContact\\nFree Postgres Database\\nGuides\\nSoftware comparisons\\nCode of Conduct\\nPrivacy Policy\\nTerms of use\\nBuilt on\\nForem\\n— the\\nopen source\\nsoftware that powers\\nDEV\\nand other inclusive communities.\\nMade with love and\\nRuby on Rails\\n. DEV Community\\n©\\n2016 - 2024.\\nWe\\'re a place where coders share, stay up-to-date and grow their careers.\\nLog in\\nCreate account', 'LangGraph: A Detailed Technical Exploration of the Next-Generation AI Workflow Framework\\nAgree & Join LinkedIn\\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nSkip to main content\\nLinkedIn\\nArticles\\nPeople\\nLearning\\nJobs\\nGames\\nJoin now\\nSign in\\nLangGraph: A Detailed Technical Exploration of the Next-Generation AI Workflow Framework\\nReport this article\\nGanesh Jagadeesan\\nGanesh Jagadeesan\\nEnterprise Data Science Specialist @Mastech Digital | NLP | NER | Deep Learning | Gen AI | MLops\\nPublished Aug 31, 2024\\n+ Follow\\nIntroduction\\nIn the rapidly advancing world of artificial intelligence (AI) and machine learning (ML), the demand for efficient, scalable, and user-friendly frameworks has never been higher. These frameworks need to support the entire lifecycle of AI models, from data preprocessing and model training to deployment and monitoring. LangGraph is a cutting-edge framework designed to meet these needs, providing a robust and flexible environment for developing and managing complex AI workflows.\\nThis article delves into the technical details of LangGraph, exploring its architecture, key features, integration capabilities, and practical applications. Whether you are an AI researcher, a data scientist, or a developer, understanding LangGraph will help you harness its full potential in your AI projects.\\nWhat is LangGraph?\\nLangGraph is an advanced AI workflow framework that integrates language models, graph-based processing, and workflow management into a cohesive platform. It is designed to simplify the development, deployment, and maintenance of complex AI systems, particularly those that involve multiple interconnected components such as natural language processing (NLP), machine learning, and data analytics.\\nLangGraph‚Äôs unique combination of language models and graph-based structures allows it to handle complex relationships and dependencies within AI workflows. This makes it particularly well-suited for tasks that require intricate data processing pipelines, multi-step reasoning, and real-time decision-making.\\nArchitecture of LangGraph\\nLangGraph‚Äôs architecture is built on three core components: language models, graph processing, and workflow orchestration. Each component plays a critical role in enabling the framework‚Äôs powerful capabilities.\\n1. Language Models\\nAt the heart of LangGraph is its integration with advanced language models. These models, such as GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), provide the foundation for understanding and generating human-like text. LangGraph leverages these models to perform a wide range of tasks, including text analysis, summarization, translation, and question answering.\\nLanguage Model Integration:\\nLangGraph seamlessly integrates with popular language models through APIs and pre-trained embeddings. This allows developers to incorporate sophisticated NLP capabilities into their workflows without the need for extensive customization.\\nCustom Language Models:\\nIn addition to using pre-trained models, LangGraph supports the development and integration of custom language models. This is particularly useful for domain-specific applications where general-purpose models may not perform optimally.\\n2. Graph-Based Processing\\nLangGraph‚Äôs graph processing engine is what sets it apart from other AI frameworks. It uses directed acyclic graphs (DAGs) to represent and manage the flow of data and tasks within an AI workflow. Each node in the graph represents a specific task or operation, while the edges define the dependencies between these tasks.\\nData Flow Management:\\nThe graph structure allows for precise control over data flow and task execution. This ensures that tasks are performed in the correct order and that intermediate data is passed efficiently between nodes.\\nParallel Processing:\\nLangGraph‚Äôs graph-based architecture enables parallel processing of tasks, significantly speeding up workflows that involve large datasets or complex computations. Tasks that are independent of each other can be executed simultaneously, reducing overall processing time.\\nError Handling and Recovery:\\nLangGraph includes robust error handling mechanisms that allow workflows to recover from failures. If a task fails, LangGraph can automatically retry the task or re-route data to an alternative path, ensuring that the workflow continues without interruption.\\n3. Workflow Orchestration\\nThe workflow orchestration layer in LangGraph is responsible for managing the execution of tasks within the graph. It provides tools for defining, scheduling, and monitoring workflows, making it easier to manage complex AI projects.\\nWorkflow Definition:\\nLangGraph allows developers to define workflows using a combination of graphical interfaces and scripting languages. This flexibility enables both visual and code-based workflow creation, catering to a wide range of users.\\nScheduling and Automation:\\nLangGraph supports the scheduling of workflows, allowing tasks to be executed at specific times or in response to certain events. This is particularly useful for recurring tasks such as data updates, model retraining, or periodic reports.\\nMonitoring and Logging:\\nThe orchestration layer includes comprehensive monitoring and logging features that track the progress of workflows in real-time. Developers can view detailed logs, set up alerts for specific events, and analyze workflow performance to identify bottlenecks or inefficiencies.\\nKey Features of LangGraph\\nLangGraph offers a wide range of features designed to simplify the development and management of AI workflows. Here are some of the most important features:\\n1. Modular Architecture\\nLangGraph‚Äôs modular architecture allows developers to build workflows from reusable components. Each component, whether it‚Äôs a language model, a data processing task, or a machine learning model, can be easily integrated into different workflows. This modularity promotes code reuse, reduces development time, and ensures consistency across projects.\\n2. Multi-Modal Support\\nLangGraph is designed to handle multi-modal data, including text, images, audio, and structured data. This makes it an ideal platform for developing AI applications that require the integration of different data types, such as sentiment analysis of social media posts, image captioning, or voice-activated systems.\\n3. Real-Time Processing\\nFor applications that require real-time decision-making, such as chatbots, recommendation engines, or fraud detection systems, LangGraph offers real-time processing capabilities. The framework is optimized for low-latency execution, ensuring that workflows respond quickly to incoming data.\\n4. Scalability\\nLangGraph is built to scale with the needs of your application. Whether you‚Äôre processing small datasets on a single machine or handling large-scale data across a distributed cluster, LangGraph can scale to meet your requirements. Its support for parallel processing and distributed computing makes it well-suited for enterprise-level AI deployments.\\n5. Integration with External Systems\\nLangGraph can easily integrate with external systems, databases, and APIs. This enables seamless interaction with other components of your IT infrastructure, such as data warehouses, cloud storage, or third-party services. This integration capability is crucial for building end-to-end AI solutions that connect with existing business processes.\\n6. Security and Compliance\\nLangGraph includes features designed to meet security and compliance requirements. It supports role-based access control (RBAC), data encryption, and audit logging, ensuring that workflows are secure and comply with industry standards. This makes LangGraph suitable for use in regulated industries such as finance, healthcare, and government.\\n7. Custom Plugins and Extensions\\nLangGraph supports the creation of custom plugins and extensions, allowing developers to add new functionality to the framework. This extensibility ensures that LangGraph can adapt to the evolving needs of AI projects and integrate with emerging technologies.\\nPractical Applications of LangGraph\\nLangGraph‚Äôs versatility and power make it applicable to a wide range of AI and ML projects. Here are some practical applications:\\n1. Natural Language Processing (NLP) Workflows\\nLangGraph is particularly well-suited for NLP applications. Its integration with advanced language models allows for the development of workflows that handle tasks such as text classification, sentiment analysis, entity recognition, and machine translation.\\nExample:\\nA content moderation system that automatically analyzes and classifies user-generated content to detect inappropriate language or spam. LangGraph can orchestrate the entire process, from data ingestion to classification and alert generation.\\n2. Automated Machine Learning (AutoML)\\nLangGraph can be used to automate the machine learning process, from data preprocessing and feature selection to model training and hyperparameter tuning. By defining these tasks as nodes in a graph, LangGraph allows for the automation of complex ML workflows that can adapt based on the data and results.\\nExample:\\nAn automated system for financial forecasting that continuously trains and updates predictive models based on new market data. LangGraph can automate the entire pipeline, ensuring that models are always up-to-date and optimized for accuracy.\\n3. Graph-Based Data Analysis\\nLangGraph‚Äôs graph processing capabilities make it ideal for applications that involve complex relationships between entities, such as social network analysis, fraud detection, or supply chain optimization. By representing data as nodes and edges in a graph, LangGraph enables powerful analytical workflows that can uncover hidden patterns and insights.\\nExample:\\nA fraud detection system that analyzes transactions and user behavior to identify suspicious activities. LangGraph can process transaction data in real-time, building a graph of interactions that can be analyzed to detect anomalies and trigger alerts.\\n4. Multi-Modal AI Systems\\nLangGraph‚Äôs support for multi-modal data allows for the creation of AI systems that combine different types of data, such as text, images, and audio. This is particularly useful for applications like multimedia content analysis, personalized recommendations, or interactive AI systems.\\nExample:\\nA personalized recommendation engine that uses text reviews, product images, and user interaction data to generate tailored product suggestions. LangGraph can orchestrate the analysis of each data type and combine the results to produce more accurate recommendations.\\n5. Intelligent Automation\\nLangGraph can be used to automate complex business processes that require decision-making based on large amounts of data. By integrating AI models with business rules and workflow automation, LangGraph enables the creation of intelligent systems that can adapt to changing conditions and optimize operations.\\nExample:\\nAn automated customer support system that uses NLP to analyze incoming queries, classify them, and route them to the appropriate department or resolve them using AI-driven responses. LangGraph can manage the entire workflow, ensuring efficient and accurate customer service.\\nIntegrating LangGraph with Other Technologies\\nLangGraph‚Äôs ability to integrate with a wide range of technologies makes it a versatile platform for building end-to-end AI solutions. Here‚Äôs how LangGraph can be integrated with other key technologies:\\n1. Cloud Services\\nLangGraph can be deployed on major cloud platforms like AWS, Google Cloud, and Azure. This allows for scalable and cost-effective deployment, leveraging cloud resources for processing and storage. Cloud integration also enables the use of cloud-based services like AWS Lambda for serverless computing or Google Cloud Storage for data storage.\\nExample:\\nDeploying LangGraph on AWS to build a scalable data processing pipeline that ingests data from S3, processes it using Lambda functions, and stores the results in a Redshift data warehouse.\\n2. Data Science Platforms\\nLangGraph can integrate with popular data science platforms such as Jupyter Notebooks, Apache Spark, and TensorFlow. This allows data scientists to develop and test models in their preferred environment before integrating them into LangGraph workflows for production deployment.\\nExample:\\nUsing Jupyter Notebooks for exploratory data analysis and model development, and then integrating the finalized model into a LangGraph workflow for automated deployment and monitoring.\\n3. APIs and Microservices\\nLangGraph supports integration with RESTful APIs and microservices, allowing it to interact with external applications and services. This is essential for building AI systems that need to interact with other parts of an organization‚Äôs IT infrastructure or with third-party services.\\nExample:\\nBuilding an AI-driven recommendation system that integrates with an e-commerce platform via REST APIs, providing real-time product suggestions to users based on their browsing history and preferences.\\n4. DevOps and MLOps\\nLangGraph can be integrated into DevOps and MLOps pipelines to automate the deployment, monitoring, and updating of AI models. This ensures that models are continuously optimized and that any issues are quickly identified and resolved.\\nExample:\\nIntegrating LangGraph with a CI/CD pipeline using Jenkins or GitLab to automate the deployment of machine learning models, with continuous monitoring and feedback loops to ensure model performance remains high.\\nSecurity and Compliance in LangGraph\\nLangGraph is designed with security and compliance in mind, making it suitable for use in industries with strict regulatory requirements.\\n1. Role-Based Access Control (RBAC)\\nLangGraph supports RBAC, allowing administrators to define roles and permissions for users. This ensures that only authorized personnel have access to sensitive data and workflows, reducing the risk of unauthorized access or data breaches.\\n2. Data Encryption\\nData processed by LangGraph can be encrypted both at rest and in transit. This ensures that sensitive information is protected from unauthorized access, meeting compliance requirements such as GDPR, HIPAA, and PCI-DSS.\\n3. Audit Logging\\nLangGraph includes comprehensive audit logging features that track all user actions and system events. These logs can be used to monitor activity, investigate incidents, and demonstrate compliance with regulatory requirements.\\n4. Compliance Certifications\\nFor organizations in regulated industries, LangGraph supports compliance with industry standards and certifications. This includes ISO/IEC 27001 for information security management and SOC 2 for service organization controls.\\nGetting Started with LangGraph\\nIf you‚Äôre ready to explore LangGraph, here‚Äôs how you can get started:\\n1. Installation and Setup\\nLangGraph can be installed on a local machine, server, or cloud environment. Installation packages and documentation are available on the official LangGraph website or through package managers like pip for Python environments.\\n2. Learning Resources\\nLangGraph provides extensive documentation, tutorials, and sample projects to help new users get up to speed. The LangGraph community also offers forums, Q&A sites, and GitHub repositories where users can share knowledge and collaborate on projects.\\n3. Creating Your First Workflow\\nStart by defining a simple workflow using LangGraph‚Äôs graphical interface or scripting language. Experiment with adding tasks, defining dependencies, and integrating external data sources. As you become more comfortable with the framework, you can explore more advanced features such as parallel processing, error handling, and custom plugins.\\n4. Scaling Your Workflows\\nAs your experience with LangGraph grows, you can begin to scale your workflows by integrating additional data sources, deploying to cloud environments, and automating tasks using the scheduling and orchestration features. Experiment with multi-modal data and real-time processing to build more complex AI systems.\\nConclusion\\nLangGraph represents a significant advancement in AI workflow management, offering a powerful combination of language models, graph-based processing, and workflow orchestration. Its modular architecture, multi-modal support, and real-time processing capabilities make it an ideal platform for developing and managing complex AI systems.\\nBy understanding the architecture, key features, and practical applications of LangGraph, developers and data scientists can harness its full potential to build scalable, efficient, and intelligent AI solutions. Whether you‚Äôre working on natural language processing, automated machine learning, or graph-based data analysis, LangGraph provides the tools and flexibility needed to succeed in today‚Äôs AI-driven world.\\nAs AI continues to evolve, frameworks like LangGraph will play a crucial role in enabling the development of next-generation AI applications, driving innovation across industries, and transforming how businesses operate.\\nLike\\nComment\\nCopy\\nLinkedIn\\nFacebook\\nTwitter\\nShare\\n1\\nTo view or add a comment,\\nsign in\\nMore articles by this author\\nNo more previous content\\nAudio to Image with LLMs: Bridging the Gap Between Sound and Vision\\nSep 19, 2024\\nUnderstanding the Differences Between Variational Autoencoders (VAE) and U-Net Architectures\\nSep 19, 2024\\nRAG vs Function Calling vs Fine-Tuning: A Detailed Comparison of Advanced LLM Techniques\\nSep 18, 2024\\nA Detailed Overview of the RAG (Retrieval-Augmented Generation) Workflow with the Latest Technology Enhancements\\nSep 18, 2024\\nCosine Similarity in Large Language Models (LLMs)\\nSep 17, 2024\\nA Comprehensive Guide to OpenAI‚Äôs Strawberry (o1): A New Era in AI Reasoning \\uf8ffüçì\\uf8ffü§ñ\\nSep 13, 2024\\nLeveraging FastAPI with Large Language Models (LLMs): A Comprehensive Guide\\nAug 31, 2024\\nThe Business Relevance of Hypothesis Testing: A Critical Tool for Decision-Making\\nAug 31, 2024\\nDeploying AI Models in Amazon SageMaker: An In-Depth Guide\\nAug 31, 2024\\nAmazon Bedrock: A Deep Dive into the Future of Generative AI\\nAug 31, 2024\\nNo more next content\\nSee all\\nSign in\\nStay updated on your professional world\\nSign in\\nBy clicking Continue to join or sign in, you agree to LinkedIn‚Äôs\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nNew to LinkedIn?\\nJoin now\\nExplore topics\\nSales\\nMarketing\\nIT Services\\nBusiness Administration\\nHR Management\\nEngineering\\nSoft Skills\\nSee All\\nLinkedIn\\n© 2024\\nAbout\\nAccessibility\\nUser Agreement\\nPrivacy Policy\\nYour California Privacy Choices\\nCookie Policy\\nCopyright Policy\\nBrand Policy\\nGuest Controls\\nCommunity Guidelines\\nÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\\n‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\\nƒåe≈°tina (Czech)\\nDansk (Danish)\\nDeutsch (German)\\nŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\\nEnglish (English)\\nEspa√±ol (Spanish)\\nŸÅÿßÿ±ÿ≥€å (Persian)\\nSuomi (Finnish)\\nFran√ßais (French)\\n‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\\nMagyar (Hungarian)\\nBahasa Indonesia (Indonesian)\\nItaliano (Italian)\\n◊¢◊ë◊®◊ô◊™ (Hebrew)\\nÊó•Êú¨Ë™û (Japanese)\\nÌïúÍµ≠Ïñ¥ (Korean)\\n‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\\nBahasa Malaysia (Malay)\\nNederlands (Dutch)\\nNorsk (Norwegian)\\n‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\\nPolski (Polish)\\nPortugu√™s (Portuguese)\\nRom√¢nƒÉ (Romanian)\\n–†—É—Å—Å–∫–∏–π (Russian)\\nSvenska (Swedish)\\n‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\\n‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\\nTagalog (Tagalog)\\nT√ºrk√ße (Turkish)\\n–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\\nTi·∫øng Vi·ªát (Vietnamese)\\nÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\\nÊ≠£È´î‰∏≠Êñá (Chinese (Traditional))\\nLanguage', 'langgraph · PyPI\\nSkip to main content\\nSwitch to mobile version\\nWarning\\nSome features may not work without JavaScript. Please try enabling it if you encounter problems.\\nSearch PyPI\\nSearch\\nHelp\\nSponsors\\nLog in\\nRegister\\nMenu\\nHelp\\nSponsors\\nLog in\\nRegister\\nSearch PyPI\\nSearch\\nlanggraph 0.2.59\\npip install langgraph\\nCopy PIP instructions\\nLatest version\\nReleased:\\nDec 11, 2024\\nBuilding stateful, multi-actor applications with LLMs\\nNavigation\\nProject description\\nRelease history\\nDownload files\\nVerified details\\nThese details have been\\nverified by PyPI\\nMaintainers\\nhwchase17\\nnfcampos\\nUnverified details\\nThese details have\\nnot\\nbeen verified by PyPI\\nProject links\\nHomepage\\nRepository\\nMeta\\nLicense:\\nMIT License (MIT)\\nRequires:\\nPython <4.0, >=3.9.0\\nClassifiers\\nLicense\\nOSI Approved :: MIT License\\nProgramming Language\\nPython :: 3\\nPython :: 3.9\\nPython :: 3.10\\nPython :: 3.11\\nPython :: 3.12\\nProject description\\nProject details\\nRelease history\\nDownload files\\nProject description\\n🦜🕸️LangGraph\\n⚡ Building language agents as graphs ⚡\\n[!NOTE]\\nLooking for the JS version? Click\\nhere\\n(\\nJS docs\\n).\\nOverview\\nLangGraph\\nis a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\\nLangGraph is inspired by\\nPregel\\nand\\nApache Beam\\n. The public interface draws inspiration from\\nNetworkX\\n. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\nLangGraph Platform\\nis infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications:\\nLangGraph Server\\n(APIs),\\nLangGraph SDKs\\n(clients for the APIs),\\nLangGraph CLI\\n(command line tool for building the server),\\nLangGraph Studio\\n(UI/debugger),\\nTo learn more about LangGraph, check out our first LangChain Academy course,\\nIntroduction to LangGraph\\n, available for free\\nhere\\n.\\nKey Features\\nCycles and Branching\\n: Implement loops and conditionals in your apps.\\nPersistence\\n: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\\nHuman-in-the-Loop\\n: Interrupt graph execution to approve or edit next action planned by the agent.\\nStreaming Support\\n: Stream outputs as they are produced by each node (including token streaming).\\nIntegration with LangChain\\n: LangGraph integrates seamlessly with\\nLangChain\\nand\\nLangSmith\\n(but does not require them).\\nLangGraph Platform\\nLangGraph Platform is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework.\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:\\nStreaming support\\n: LangGraph Server provides\\nmultiple streaming modes\\noptimized for various application needs\\nBackground runs\\n: Runs agents asynchronously in the background\\nSupport for long running agents\\n: Infrastructure that can handle long running processes\\nDouble texting\\n: Handle the case where you get two messages from the user before the agent can respond\\nHandle burstiness\\n: Task queue for ensuring requests are handled consistently without loss, even under heavy loads\\nInstallation\\npip\\ninstall\\n-U\\nlanggraph\\nExample\\nOne of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\\nLet\\'s take a look at a simple example of an agent that can use a search tool.\\npip\\ninstall\\nlangchain-anthropic\\nexport\\nANTHROPIC_API_KEY\\n=\\nsk-...\\nOptionally, we can set up\\nLangSmith\\nfor best-in-class observability.\\nexport\\nLANGSMITH_TRACING\\n=\\ntrue\\nexport\\nLANGSMITH_API_KEY\\n=\\nlsv2_sk_...\\nfrom\\ntyping\\nimport\\nAnnotated\\n,\\nLiteral\\n,\\nTypedDict\\nfrom\\nlangchain_core.messages\\nimport\\nHumanMessage\\nfrom\\nlangchain_anthropic\\nimport\\nChatAnthropic\\nfrom\\nlangchain_core.tools\\nimport\\ntool\\nfrom\\nlanggraph.checkpoint.memory\\nimport\\nMemorySaver\\nfrom\\nlanggraph.graph\\nimport\\nEND\\n,\\nSTART\\n,\\nStateGraph\\n,\\nMessagesState\\nfrom\\nlanggraph.prebuilt\\nimport\\nToolNode\\n# Define the tools for the agent to use\\n@tool\\ndef\\nsearch\\n(\\nquery\\n:\\nstr\\n):\\n\"\"\"Call to surf the web.\"\"\"\\n# This is a placeholder, but don\\'t tell the LLM that...\\nif\\n\"sf\"\\nin\\nquery\\n.\\nlower\\n()\\nor\\n\"san francisco\"\\nin\\nquery\\n.\\nlower\\n():\\nreturn\\n\"It\\'s 60 degrees and foggy.\"\\nreturn\\n\"It\\'s 90 degrees and sunny.\"\\ntools\\n=\\n[\\nsearch\\n]\\ntool_node\\n=\\nToolNode\\n(\\ntools\\n)\\nmodel\\n=\\nChatAnthropic\\n(\\nmodel\\n=\\n\"claude-3-5-sonnet-20240620\"\\n,\\ntemperature\\n=\\n0\\n)\\n.\\nbind_tools\\n(\\ntools\\n)\\n# Define the function that determines whether to continue or not\\ndef\\nshould_continue\\n(\\nstate\\n:\\nMessagesState\\n)\\n->\\nLiteral\\n[\\n\"tools\"\\n,\\nEND\\n]:\\nmessages\\n=\\nstate\\n[\\n\\'messages\\'\\n]\\nlast_message\\n=\\nmessages\\n[\\n-\\n1\\n]\\n# If the LLM makes a tool call, then we route to the \"tools\" node\\nif\\nlast_message\\n.\\ntool_calls\\n:\\nreturn\\n\"tools\"\\n# Otherwise, we stop (reply to the user)\\nreturn\\nEND\\n# Define the function that calls the model\\ndef\\ncall_model\\n(\\nstate\\n:\\nMessagesState\\n):\\nmessages\\n=\\nstate\\n[\\n\\'messages\\'\\n]\\nresponse\\n=\\nmodel\\n.\\ninvoke\\n(\\nmessages\\n)\\n# We return a list, because this will get added to the existing list\\nreturn\\n{\\n\"messages\"\\n:\\n[\\nresponse\\n]}\\n# Define a new graph\\nworkflow\\n=\\nStateGraph\\n(\\nMessagesState\\n)\\n# Define the two nodes we will cycle between\\nworkflow\\n.\\nadd_node\\n(\\n\"agent\"\\n,\\ncall_model\\n)\\nworkflow\\n.\\nadd_node\\n(\\n\"tools\"\\n,\\ntool_node\\n)\\n# Set the entrypoint as `agent`\\n# This means that this node is the first one called\\nworkflow\\n.\\nadd_edge\\n(\\nSTART\\n,\\n\"agent\"\\n)\\n# We now add a conditional edge\\nworkflow\\n.\\nadd_conditional_edges\\n(\\n# First, we define the start node. We use `agent`.\\n# This means these are the edges taken after the `agent` node is called.\\n\"agent\"\\n,\\n# Next, we pass in the function that will determine which node is called next.\\nshould_continue\\n,\\n)\\n# We now add a normal edge from `tools` to `agent`.\\n# This means that after `tools` is called, `agent` node is called next.\\nworkflow\\n.\\nadd_edge\\n(\\n\"tools\"\\n,\\n\\'agent\\'\\n)\\n# Initialize memory to persist state between graph runs\\ncheckpointer\\n=\\nMemorySaver\\n()\\n# Finally, we compile it!\\n# This compiles it into a LangChain Runnable,\\n# meaning you can use it as you would any other runnable.\\n# Note that we\\'re (optionally) passing the memory when compiling the graph\\napp\\n=\\nworkflow\\n.\\ncompile\\n(\\ncheckpointer\\n=\\ncheckpointer\\n)\\n# Use the Runnable\\nfinal_state\\n=\\napp\\n.\\ninvoke\\n(\\n{\\n\"messages\"\\n:\\n[\\nHumanMessage\\n(\\ncontent\\n=\\n\"what is the weather in sf\"\\n)]},\\nconfig\\n=\\n{\\n\"configurable\"\\n:\\n{\\n\"thread_id\"\\n:\\n42\\n}}\\n)\\nfinal_state\\n[\\n\"messages\"\\n][\\n-\\n1\\n]\\n.\\ncontent\\n\"Based on the search results, I can tell you that the current weather in San Francisco is:\\\\n\\\\nTemperature: 60 degrees Fahrenheit\\\\nConditions: Foggy\\\\n\\\\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60°F (about 15.5°C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as \"Karl the Fog\" by locals, is a characteristic feature of San Francisco\\\\\\'s weather, particularly in the mornings and evenings.\\\\n\\\\nIs there anything else you\\\\\\'d like to know about the weather in San Francisco or any other location?\"\\nNow when we pass the same\\n\"thread_id\"\\n, the conversation context is retained via the saved state (i.e. stored list of messages)\\nfinal_state\\n=\\napp\\n.\\ninvoke\\n(\\n{\\n\"messages\"\\n:\\n[\\nHumanMessage\\n(\\ncontent\\n=\\n\"what about ny\"\\n)]},\\nconfig\\n=\\n{\\n\"configurable\"\\n:\\n{\\n\"thread_id\"\\n:\\n42\\n}}\\n)\\nfinal_state\\n[\\n\"messages\"\\n][\\n-\\n1\\n]\\n.\\ncontent\\n\"Based on the search results, I can tell you that the current weather in New York City is:\\\\n\\\\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\\\\nConditions: Sunny\\\\n\\\\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\\\\n\\\\n1. The temperature of 90°F is quite hot, typical of summer weather in New York City.\\\\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\\\\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\\\\n\\\\nIt\\'s interesting to see the stark contrast between San Francisco\\'s mild, foggy weather and New York\\'s hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\\\\n\\\\nIs there anything else you\\'d like to know about the weather in New York or any other location?\"\\nStep-by-step Breakdown\\nInitialize the model and tools.\\nwe use\\nChatAnthropic\\nas our LLM.\\nNOTE:\\nwe need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the\\n.bind_tools()\\nmethod.\\nwe define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that\\nhere\\n.\\nInitialize graph with state.\\nwe initialize graph (\\nStateGraph\\n) by passing state schema (in our case\\nMessagesState\\n)\\nMessagesState\\nis a prebuilt state schema that has one attribute -- a list of LangChain\\nMessage\\nobjects, as well as logic for merging the updates from each node into the state\\nDefine graph nodes.\\nThere are two main nodes we need:\\nThe\\nagent\\nnode: responsible for deciding what (if any) actions to take.\\nThe\\ntools\\nnode that invokes tools: if the agent decides to take an action, this node will then execute that action.\\nDefine entry point and graph edges.\\nFirst, we need to set the entry point for graph execution -\\nagent\\nnode.\\nThen we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph\\'s state (\\nMessageState\\n). In our case, the destination is not known until the agent (LLM) decides.\\nConditional edge: after the agent is called, we should either:\\na. Run tools if the agent said to take an action, OR\\nb. Finish (respond to the user) if the agent did not ask to run tools\\nNormal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\\nCompile the graph.\\nWhen we compile the graph, we turn it into a LangChain\\nRunnable\\n, which automatically enables calling\\n.invoke()\\n,\\n.stream()\\nand\\n.batch()\\nwith your inputs\\nWe can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use\\nMemorySaver\\n- a simple in-memory checkpointer\\nExecute the graph.\\nLangGraph adds the input message to the internal state, then passes the state to the entrypoint node,\\n\"agent\"\\n.\\nThe\\n\"agent\"\\nnode executes, invoking the chat model.\\nThe chat model returns an\\nAIMessage\\n. LangGraph adds this to the state.\\nGraph cycles the following steps until there are no more\\ntool_calls\\non\\nAIMessage\\n:\\nIf\\nAIMessage\\nhas\\ntool_calls\\n,\\n\"tools\"\\nnode executes\\nThe\\n\"agent\"\\nnode executes again and returns\\nAIMessage\\nExecution progresses to the special\\nEND\\nvalue and outputs the final state.\\nAnd as a result, we get a list of all our chat messages as output.\\nDocumentation\\nTutorials\\n: Learn to build with LangGraph through guided examples.\\nHow-to Guides\\n: Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.\\nConceptual Guides\\n: In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.\\nAPI Reference\\n: Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.\\nLangGraph Platform\\n: LangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.\\nContributing\\nFor more information on how to contribute, see\\nhere\\n.\\nProject details\\nVerified details\\nThese details have been\\nverified by PyPI\\nMaintainers\\nhwchase17\\nnfcampos\\nUnverified details\\nThese details have\\nnot\\nbeen verified by PyPI\\nProject links\\nHomepage\\nRepository\\nMeta\\nLicense:\\nMIT License (MIT)\\nRequires:\\nPython <4.0, >=3.9.0\\nClassifiers\\nLicense\\nOSI Approved :: MIT License\\nProgramming Language\\nPython :: 3\\nPython :: 3.9\\nPython :: 3.10\\nPython :: 3.11\\nPython :: 3.12\\nRelease history\\nRelease notifications\\n|\\nRSS feed\\nThis version\\n0.2.59\\nDec 11, 2024\\n0.2.58\\nDec 10, 2024\\n0.2.57\\nDec 10, 2024\\n0.2.56\\nDec 5, 2024\\n0.2.55\\nDec 5, 2024\\n0.2.54\\nDec 3, 2024\\n0.2.53\\nNov 21, 2024\\n0.2.52\\nNov 19, 2024\\n0.2.51\\nNov 19, 2024\\n0.2.50\\nNov 15, 2024\\n0.2.49\\nNov 15, 2024\\n0.2.48\\nNov 14, 2024\\n0.2.47\\nNov 13, 2024\\n0.2.46\\nNov 13, 2024\\n0.2.45\\nNov 4, 2024\\n0.2.44\\nNov 2, 2024\\n0.2.43\\nOct 31, 2024\\n0.2.42\\nOct 31, 2024\\n0.2.41\\nOct 31, 2024\\n0.2.40\\nOct 30, 2024\\n0.2.39\\nOct 18, 2024\\n0.2.38\\nOct 15, 2024\\n0.2.37\\nOct 15, 2024\\n0.2.36\\nOct 14, 2024\\n0.2.35\\nOct 9, 2024\\n0.2.34\\nOct 2, 2024\\n0.2.33\\nOct 2, 2024\\n0.2.32\\nOct 1, 2024\\n0.2.31\\nyanked\\nSep 30, 2024\\nReason this release was yanked:\\nUses internal APIs of upstream package.\\n0.2.30\\nyanked\\nSep 30, 2024\\nReason this release was yanked:\\nUses internal APIs of upstream package.\\n0.2.29\\nyanked\\nSep 30, 2024\\nReason this release was yanked:\\nUses internal APIs of upstream package.\\n0.2.28\\nSep 25, 2024\\n0.2.27\\nSep 24, 2024\\n0.2.26\\nSep 24, 2024\\n0.2.25\\nSep 23, 2024\\n0.2.24\\nSep 23, 2024\\n0.2.23\\nSep 20, 2024\\n0.2.22\\nSep 16, 2024\\n0.2.21\\nSep 13, 2024\\n0.2.20\\nSep 13, 2024\\n0.2.19\\nSep 6, 2024\\n0.2.18\\nSep 6, 2024\\n0.2.17\\nSep 5, 2024\\n0.2.16\\nSep 1, 2024\\n0.2.15\\nAug 30, 2024\\n0.2.14\\nAug 24, 2024\\n0.2.13\\nAug 23, 2024\\n0.2.12\\nAug 22, 2024\\n0.2.11\\nAug 22, 2024\\n0.2.10\\nAug 21, 2024\\n0.2.9\\nAug 21, 2024\\n0.2.8\\nAug 21, 2024\\n0.2.7\\nAug 21, 2024\\n0.2.7a0\\npre-release\\nAug 21, 2024\\n0.2.6\\nAug 21, 2024\\n0.2.5\\nAug 21, 2024\\n0.2.5a0\\npre-release\\nAug 20, 2024\\n0.2.4\\nAug 15, 2024\\n0.2.3\\nAug 8, 2024\\n0.2.2\\nAug 7, 2024\\n0.2.1\\nAug 7, 2024\\n0.2.0\\nAug 7, 2024\\n0.1.19\\nAug 1, 2024\\n0.1.18\\nyanked\\nJul 31, 2024\\nReason this release was yanked:\\nbroken\\n0.1.17\\nJul 31, 2024\\n0.1.16\\nJul 29, 2024\\n0.1.15\\nJul 26, 2024\\n0.1.14\\nJul 24, 2024\\n0.1.13\\nJul 24, 2024\\n0.1.12\\nJul 24, 2024\\n0.1.11\\nJul 23, 2024\\n0.1.10\\nJul 23, 2024\\n0.1.9\\nJul 18, 2024\\n0.1.8\\nJul 12, 2024\\n0.1.7\\nJul 10, 2024\\n0.1.6\\nJul 9, 2024\\n0.1.5\\nJul 1, 2024\\n0.1.4\\nJun 28, 2024\\n0.1.3\\nJun 28, 2024\\n0.1.2\\nJun 26, 2024\\n0.1.1\\nJun 22, 2024\\n0.0.69\\nJun 14, 2024\\n0.0.68\\nJun 13, 2024\\n0.0.67\\nJun 13, 2024\\n0.0.66\\nJun 10, 2024\\n0.0.65\\nJun 7, 2024\\n0.0.64\\nJun 6, 2024\\n0.0.63\\nJun 5, 2024\\n0.0.62\\nJun 3, 2024\\n0.0.61\\nJun 2, 2024\\n0.0.60\\nMay 31, 2024\\n0.0.59\\nMay 30, 2024\\n0.0.58\\nMay 30, 2024\\n0.0.57\\nMay 30, 2024\\n0.0.56\\nMay 29, 2024\\n0.0.55\\nMay 22, 2024\\n0.0.54\\nMay 21, 2024\\n0.0.53\\nMay 20, 2024\\n0.0.52\\nMay 20, 2024\\n0.0.51\\nMay 20, 2024\\n0.0.50\\nMay 17, 2024\\n0.0.49\\nMay 14, 2024\\n0.0.48\\nMay 8, 2024\\n0.0.47\\nMay 8, 2024\\n0.0.46\\nMay 7, 2024\\n0.0.45\\nMay 7, 2024\\n0.0.44\\nMay 3, 2024\\n0.0.43\\nMay 3, 2024\\n0.0.42\\nMay 3, 2024\\n0.0.41\\nMay 3, 2024\\n0.0.40\\nApr 30, 2024\\n0.0.39\\nApr 25, 2024\\n0.0.38\\nApr 17, 2024\\n0.0.37\\nApr 12, 2024\\n0.0.36\\nApr 11, 2024\\n0.0.35\\nApr 11, 2024\\n0.0.34\\nApr 10, 2024\\n0.0.33\\nApr 10, 2024\\n0.0.32\\nApr 4, 2024\\n0.0.31\\nApr 2, 2024\\n0.0.30\\nMar 22, 2024\\n0.0.29\\nMar 20, 2024\\n0.0.28\\nMar 13, 2024\\n0.0.27\\nMar 13, 2024\\n0.0.26\\nFeb 22, 2024\\n0.0.25\\nFeb 22, 2024\\n0.0.24\\nFeb 8, 2024\\n0.0.23\\nFeb 4, 2024\\n0.0.22\\nFeb 4, 2024\\n0.0.21\\nJan 31, 2024\\n0.0.20\\nJan 27, 2024\\n0.0.19\\nJan 23, 2024\\n0.0.18\\nJan 23, 2024\\n0.0.17\\nJan 23, 2024\\n0.0.16\\nJan 21, 2024\\n0.0.15\\nJan 19, 2024\\n0.0.14\\nJan 18, 2024\\n0.0.13\\nJan 17, 2024\\n0.0.12\\nJan 17, 2024\\n0.0.11\\nJan 16, 2024\\n0.0.10\\nJan 9, 2024\\n0.0.9\\nJan 8, 2024\\n0.0.8\\nyanked\\nJan 8, 2024\\nDownload files\\nDownload the file for your platform. If you\\'re not sure which to choose, learn more about\\ninstalling packages\\n.\\nSource Distribution\\nlanggraph-0.2.59.tar.gz\\n(116.8 kB\\nview details\\n)\\nUploaded\\nDec 11, 2024\\nSource\\nBuilt Distribution\\nlanggraph-0.2.59-py3-none-any.whl\\n(135.8 kB\\nview details\\n)\\nUploaded\\nDec 11, 2024\\nPython 3\\nFile details\\nDetails for the file\\nlanggraph-0.2.59.tar.gz\\n.\\nFile metadata\\nDownload URL:\\nlanggraph-0.2.59.tar.gz\\nUpload date:\\nDec 11, 2024\\nSize: 116.8 kB\\nTags: Source\\nUploaded using Trusted Publishing? Yes\\nUploaded via: twine/6.0.1 CPython/3.12.8\\nFile hashes\\nHashes for langgraph-0.2.59.tar.gz\\nAlgorithm\\nHash digest\\nSHA256\\n61cb5dd409878641fe8a12e9d7928a7524caa3e54ab12bc883feeaba4a5ba618\\nCopy\\nMD5\\nbb66301be96489426c1729fdbefb32b7\\nCopy\\nBLAKE2b-256\\n02cb096d2c2affa8edb9aecf9bd88b7a68e7956daf15e1d7886821c08567837f\\nCopy\\nSee more details on using hashes here.\\nFile details\\nDetails for the file\\nlanggraph-0.2.59-py3-none-any.whl\\n.\\nFile metadata\\nDownload URL:\\nlanggraph-0.2.59-py3-none-any.whl\\nUpload date:\\nDec 11, 2024\\nSize: 135.8 kB\\nTags: Python 3\\nUploaded using Trusted Publishing? Yes\\nUploaded via: twine/6.0.1 CPython/3.12.8\\nFile hashes\\nHashes for langgraph-0.2.59-py3-none-any.whl\\nAlgorithm\\nHash digest\\nSHA256\\n9b2d1331bbdcea96cfffde8e88700776118f4583d15f6a3423fc4482fe84ae56\\nCopy\\nMD5\\nb2577ceea7051b918306092cd923c97f\\nCopy\\nBLAKE2b-256\\n3f765ca524dcd8f98bc77bf3feb7216b2908f91c1660e6304a73e363e1381420\\nCopy\\nSee more details on using hashes here.\\nHelp\\nInstalling packages\\nUploading packages\\nUser guide\\nProject name retention\\nFAQs\\nAbout PyPI\\nPyPI Blog\\nInfrastructure dashboard\\nStatistics\\nLogos & trademarks\\nOur sponsors\\nContributing to PyPI\\nBugs and feedback\\nContribute on GitHub\\nTranslate PyPI\\nSponsor PyPI\\nDevelopment credits\\nUsing PyPI\\nCode of conduct\\nReport security issue\\nPrivacy Notice\\nTerms of Use\\nAcceptable Use Policy\\nStatus:\\nall systems operational\\nDeveloped and maintained by the Python community, for the Python community.\\nDonate today!\\n\"PyPI\", \"Python Package Index\", and the blocks logos are registered\\ntrademarks\\nof the\\nPython Software Foundation\\n.\\n© 2024\\nPython Software Foundation\\nSite map\\nSwitch to desktop version\\nEnglish\\nespañol\\nfrançais\\n日本語\\nportuguês (Brasil)\\nукраїнська\\nΕλληνικά\\nDeutsch\\n中文 (简体)\\n中文 (繁體)\\nрусский\\nעברית\\nEsperanto\\nSupported by\\nAWS\\nCloud computing and Security Sponsor\\nDatadog\\nMonitoring\\nFastly\\nCDN\\nGoogle\\nDownload Analytics\\nMicrosoft\\nPSF Sponsor\\nPingdom\\nMonitoring\\nSentry\\nError logging\\nStatusPage\\nStatus page']\u001b[0m\u001b[32;1m\u001b[1;3mThought: I have gathered a substantial amount of information about LangGraph, including its creators and key features. Now, I will summarize this information to provide a detailed answer to the original query.\n","\n","Final Answer: \n","LangGraph is an advanced library developed by LangChain Inc., designed to enhance the capabilities of applications powered by Large Language Models (LLMs). It builds upon the LangChain framework and introduces features that allow for the creation of stateful, multi-actor applications with cyclic computation capabilities.\n","\n","### Key Features of LangGraph:\n","1. **Cyclic Computation**: Unlike traditional Directed Acyclic Graphs (DAGs) used in LangChain, LangGraph allows for cycles in the graph, enabling more complex and agent-like behaviors in AI applications.\n","\n","2. **State Management**: LangGraph maintains a stateful graph where the state represents the context or memory that is updated as computations progress. This allows for dynamic decision-making based on accumulated data.\n","\n","3. **Nodes and Edges**: \n","   - **Nodes** represent individual computation steps or functions, which can be customized to perform various tasks.\n","   - **Edges** connect these nodes and define the flow of computation, supporting conditional logic to change execution paths based on the current state.\n","\n","4. **Automatic State Management**: The framework automatically manages the state across interactions, ensuring that context is preserved, which is crucial for intelligent responses.\n","\n","5. **Agent Coordination**: LangGraph facilitates the coordination of multiple agents, allowing for efficient information exchange and execution of complex workflows.\n","\n","6. **Scalability and Fault Tolerance**: It is designed to be robust and reliable, making it suitable for enterprise-level applications.\n","\n","7. **Integration with LangChain**: LangGraph seamlessly integrates with existing LangChain components, allowing developers to leverage established tools and functionalities.\n","\n","LangGraph is particularly useful for building sophisticated AI agents capable of performing complex, multi-step tasks while adapting to new information dynamically. It is a powerful tool for developers looking to create advanced AI applications that require more than simple retrieval-augmented generation capabilities.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","display(Markdown(response['output']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"id":"8DDP9qSLvsK6","outputId":"dd9ba9ef-2960-4838-abaa-2b299a4eeb17","executionInfo":{"status":"ok","timestamp":1734090941539,"user_tz":-330,"elapsed":483,"user":{"displayName":"Dipanjan “DJ” Sarkar","userId":"05135987707016476934"}}},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Thought: I have gathered a substantial amount of information about LangGraph, including its creators and key features. Now, I will summarize this information to provide a detailed answer to the original query.\n\nFinal Answer: \nLangGraph is an advanced library developed by LangChain Inc., designed to enhance the capabilities of applications powered by Large Language Models (LLMs). It builds upon the LangChain framework and introduces features that allow for the creation of stateful, multi-actor applications with cyclic computation capabilities.\n\n### Key Features of LangGraph:\n1. **Cyclic Computation**: Unlike traditional Directed Acyclic Graphs (DAGs) used in LangChain, LangGraph allows for cycles in the graph, enabling more complex and agent-like behaviors in AI applications.\n\n2. **State Management**: LangGraph maintains a stateful graph where the state represents the context or memory that is updated as computations progress. This allows for dynamic decision-making based on accumulated data.\n\n3. **Nodes and Edges**: \n   - **Nodes** represent individual computation steps or functions, which can be customized to perform various tasks.\n   - **Edges** connect these nodes and define the flow of computation, supporting conditional logic to change execution paths based on the current state.\n\n4. **Automatic State Management**: The framework automatically manages the state across interactions, ensuring that context is preserved, which is crucial for intelligent responses.\n\n5. **Agent Coordination**: LangGraph facilitates the coordination of multiple agents, allowing for efficient information exchange and execution of complex workflows.\n\n6. **Scalability and Fault Tolerance**: It is designed to be robust and reliable, making it suitable for enterprise-level applications.\n\n7. **Integration with LangChain**: LangGraph seamlessly integrates with existing LangChain components, allowing developers to leverage established tools and functionalities.\n\nLangGraph is particularly useful for building sophisticated AI agents capable of performing complex, multi-step tasks while adapting to new information dynamically. It is a powerful tool for developers looking to create advanced AI applications that require more than simple retrieval-augmented generation capabilities."},"metadata":{}}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}